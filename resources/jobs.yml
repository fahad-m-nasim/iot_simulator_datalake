# Job Definitions for IoT Streaming Pipeline
# 
# ALL JOBS USE JOB CLUSTERS ONLY - NO INTERACTIVE CLUSTERS
# Job clusters are ephemeral and only run when jobs are triggered
#
# Cost Optimization:
# - Single node m5d.large (2 vCPU, 8GB)
# - 100% SPOT instances
# - NO PHOTON
# - Auto-terminates immediately after job completes

resources:
  jobs:
    # Job 1: Trigger DLT pipeline for ingestion
    iot_ingestion_job:
      name: "iot-ingestion-${bundle.target}"
      description: "Triggers DLT pipeline to ingest IoT and CDC data from S3"
      
      tasks:
        - task_key: "trigger_dlt_pipeline"
          # DLT pipeline creates its own job cluster
          pipeline_task:
            pipeline_id: ${resources.pipelines.iot_streaming_pipeline.id}
            full_refresh: false
          
          timeout_seconds: 1800
      
      # Schedule: Run hourly (configurable per environment)
      schedule:
        quartz_cron_expression: "0 0 * * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"  # Paused by default, run manually
      
      queue:
        enabled: true
      
      tags:
        Project: "iot-streaming-pipeline"
        Component: "ingestion"
        Environment: "${bundle.target}"
      
      max_concurrent_runs: 1
    
    # Job 2: Run dbt transformations
    dbt_transformation_job:
      name: "dbt-transformations-${bundle.target}"
      description: "Runs dbt models to transform Bronze -> Silver -> Gold"
      
      # Define job cluster for dbt tasks - SINGLE NODE, SPOT, NO PHOTON
      job_clusters:
        - job_cluster_key: "dbt_cluster"
          new_cluster:
            spark_version: "17.3.x-scala2.13"
            node_type_id: "m5d.large"  # 2 vCPU, 8GB RAM
            num_workers: 0  # SINGLE NODE
            aws_attributes:
              availability: SPOT
              zone_id: auto
              spot_bid_price_percent: 100
              first_on_demand: 0  # ALL SPOT
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
              "spark.databricks.delta.optimizeWrite.enabled": "true"
              "spark.sql.shuffle.partitions": "4"
            custom_tags:
              ResourceClass: "SingleNode"
              Project: "iot-streaming-pipeline"
              Component: "dbt-job-cluster"
              Environment: "${bundle.target}"
      
      tasks:
        # Task 1: Run dbt deps
        - task_key: "dbt_deps"
          
          notebook_task:
            notebook_path: ../notebooks/jobs/run_dbt_deps.py
            base_parameters:
              catalog: ${var.catalog}
              schema_prefix: ${var.schema_prefix}
          
          # Use JOB CLUSTER (not interactive)
          job_cluster_key: "dbt_cluster"
          
          timeout_seconds: 600
          retry_on_timeout: false
          max_retries: 1
        
        # Task 2: Run dbt build (models + tests)
        - task_key: "dbt_build"
          depends_on:
            - task_key: "dbt_deps"
          
          notebook_task:
            notebook_path: ../notebooks/jobs/run_dbt_build.py
            base_parameters:
              catalog: ${var.catalog}
              schema_prefix: ${var.schema_prefix}
              full_refresh: "false"
          
          job_cluster_key: "dbt_cluster"
          
          timeout_seconds: 1800
          retry_on_timeout: false
          max_retries: 2
        
        # Task 3: Run dbt tests
        - task_key: "dbt_test"
          depends_on:
            - task_key: "dbt_build"
          
          notebook_task:
            notebook_path: ../notebooks/jobs/run_dbt_test.py
            base_parameters:
              catalog: ${var.catalog}
              schema_prefix: ${var.schema_prefix}
          
          job_cluster_key: "dbt_cluster"
          
          timeout_seconds: 900
          retry_on_timeout: false
          max_retries: 1
      
      schedule:
        quartz_cron_expression: "0 30 * * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"  # Paused by default
      
      queue:
        enabled: true
      
      tags:
        Project: "iot-streaming-pipeline"
        Component: "transformation"
        Environment: "${bundle.target}"
      
      max_concurrent_runs: 1
    
    # Job 3: Data quality checks
    data_quality_job:
      name: "data-quality-checks-${bundle.target}"
      description: "Runs data quality checks on Gold layer tables"
      
      # Job cluster for quality checks
      job_clusters:
        - job_cluster_key: "quality_cluster"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "m5d.large"
            num_workers: 0
            aws_attributes:
              availability: SPOT
              zone_id: auto
              spot_bid_price_percent: 100
              first_on_demand: 0
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
            custom_tags:
              ResourceClass: "SingleNode"
              Project: "iot-streaming-pipeline"
              Component: "quality-job-cluster"
              Environment: "${bundle.target}"
      
      tasks:
        - task_key: "quality_checks"
          
          notebook_task:
            notebook_path: ../notebooks/jobs/run_data_quality.py
            base_parameters:
              catalog: ${var.catalog}
              schema_prefix: ${var.schema_prefix}
          
          job_cluster_key: "quality_cluster"
          
          timeout_seconds: 600
      
      schedule:
        quartz_cron_expression: "0 0 6 * * ?"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      
      tags:
        Project: "iot-streaming-pipeline"
        Component: "quality"
        Environment: "${bundle.target}"
      
      max_concurrent_runs: 1
