# Delta Live Tables Pipeline for Streaming Ingestion
# Uses Auto Loader to ingest from Unity Catalog Volume into streaming tables
#
# COST OPTIMIZED:
# - SINGLE NODE (num_workers: 0)
# - m5d.large (2 vCPU, 8GB RAM)
# - 100% SPOT instances
# - NO PHOTON
#
# Architecture:
# S3 -> Lambda Sync -> UC Volume -> Auto Loader -> Bronze Streaming Tables -> Silver/Gold via dbt
#
# NOTE: DLT cluster names are auto-generated by Databricks as 'dlt-execution-{pipeline_id}'
# Custom naming is not supported, but custom tags help identify clusters in the console.

resources:
  pipelines:
    # Main DLT pipeline for IoT data ingestion
    iot_streaming_pipeline:
      name: "iot-streaming-ingestion-${bundle.target}"
      
      # IMPORTANT: Development mode keeps cluster running for rapid iteration.
      # Set to false in prod to auto-terminate cluster after pipeline completes.
      # In dev, manually stop the pipeline to release the cluster.
      development: false  # Changed to false for auto-termination
      
      # Triggered mode for cost savings (not continuous)
      continuous: false
      
      # Pipeline configuration
      channel: "CURRENT"
      
      # Target schema for tables
      catalog: ${var.catalog}
      target: bronze_${var.schema_prefix}
      
      # Pipeline libraries - notebooks that define streaming tables
      libraries:
        - notebook:
            path: ../notebooks/bronze/ingest_iot_events.py
        - notebook:
            path: ../notebooks/bronze/ingest_cdc_tables.py
      
      # SINGLE NODE cluster configuration for DLT - MINIMAL COST
      clusters:
        - label: "default"  # DLT only allows: default, updates, maintenance
          # SINGLE NODE - no workers
          num_workers: 0
          node_type_id: "m5d.large"  # 2 vCPU, 8GB RAM
          
          # 100% SPOT instances
          aws_attributes:
            availability: SPOT
            zone_id: auto
            spot_bid_price_percent: 100
            first_on_demand: 0  # ALL spot
          
          spark_conf:
            "spark.databricks.cluster.profile": "singleNode"
            "spark.master": "local[*]"
            "spark.databricks.delta.optimizeWrite.enabled": "true"
            "spark.databricks.delta.autoCompact.enabled": "true"
            "spark.sql.shuffle.partitions": "4"
          
          # Use custom tags to identify clusters in the Databricks console
          # DLT cluster names are auto-generated as 'dlt-execution-{pipeline_id}'
          custom_tags:
            ResourceClass: "SingleNode"
            Project: "iot-streaming-pipeline"
            Component: "dlt-bronze"
            Environment: "${bundle.target}"
            PipelineName: "iot-streaming-ingestion"
            ClusterPurpose: "bronze-ingestion"
      
      # NO PHOTON - cost savings
      photon: false
      
      # Configuration for Auto Loader
      configuration:
        # Schema evolution
        "spark.databricks.delta.schema.autoMerge.enabled": "true"
