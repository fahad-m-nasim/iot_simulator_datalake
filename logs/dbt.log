==================== 2025-12-26T04:41:16.710927Z | 019b58f6-04a4-7752-b4f0-6941a88f54d2 ====================
04:41:16.711193 [info ]: dbt-fusion 2.0.0-preview.92
04:41:16.711643 [info ]: Started loading project
04:41:16.713672 [info ]: Finished loading project [  0.00s]
04:41:16.713985 [error]: dbt1005: No profile specified in dbt_project.yml
04:41:16.823923 [info ]: 
==================== Execution Summary =====================
Finished 'debug' with 1 error [112ms]
==================== 2025-12-26T04:41:50.952542Z | 019b58f6-8a67-7aa3-97c6-2f3b1e48f28b ====================
04:41:50.952590 [info ]: dbt-fusion 2.0.0-preview.92
04:41:50.952723 [info ]: Started loading project
04:41:50.953071 [info ]: Finished loading project [  0.00s]
04:41:50.953130 [error]: dbt1005: No profile specified in dbt_project.yml
04:41:51.050658 [info ]: 
==================== Execution Summary =====================
Finished 'debug' with 1 error [98ms]
==================== 2025-12-26T04:44:19.275031Z | 019b58f8-cdca-7921-ad9b-a97ee48bb91e ====================
04:44:19.275073 [info ]: dbt-fusion 2.0.0-preview.92
04:44:19.275154 [info ]: Started loading project
04:44:19.277141 [info ]: Loading profiles.yml
04:44:19.278549 [info ]: Finished loading project [  0.00s]
04:44:19.278652 [error]: dbt1501: Failed to eval the compiled Jinja expression invalid operation: 'env_var': environment variable 'DATABRICKS_CLIENT_ID' not found
(in :1:2)
  --> profiles.yml:11:19
04:44:19.369190 [info ]: 
==================== Execution Summary =====================
Finished 'debug' with 1 error [94ms]
[0m15:54:52.288905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043a0ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057af750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057af9d0>]}


============================== 15:54:52.291191 | 505011ea-1d52-44c4-8328-2b605597202f ==============================
[0m15:54:52.291191 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:54:52.291460 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'target_path': 'None', 'introspect': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt run', 'debug': 'False', 'log_format': 'default', 'warn_error': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'static_parser': 'True', 'quiet': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'use_colors': 'True'}
[0m15:54:52.294142 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_CLIENT_ID'
[0m15:54:52.654507 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.39269626, "process_in_blocks": "0", "process_kernel_time": 0.074326, "process_mem_max_rss": "114049024", "process_out_blocks": "0", "process_user_time": 0.439213}
[0m15:54:52.655684 [debug] [MainThread]: Command `dbt run` failed at 15:54:52.655538 after 0.39 seconds
[0m15:54:52.656229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c7820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057edfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10604d8c0>]}
[0m15:54:52.656729 [debug] [MainThread]: Flushing usage events
[0m15:54:53.903966 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:55:02.172579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ee4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110caf750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110caf9d0>]}


============================== 15:55:02.175241 | aedb76cd-e1fa-4e51-bb0d-4ad2cbdd0ecc ==============================
[0m15:55:02.175241 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:55:02.175477 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'write_json': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'static_parser': 'True', 'empty': 'False', 'no_print': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'introspect': 'True', 'quiet': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'target_path': 'None', 'use_colors': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'printer_width': '80'}
[0m15:55:02.177559 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_CLIENT_ID'
[0m15:55:02.178571 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.029661292, "process_in_blocks": "0", "process_kernel_time": 0.065529, "process_mem_max_rss": "112459776", "process_out_blocks": "0", "process_user_time": 0.439249}
[0m15:55:02.178742 [debug] [MainThread]: Command `dbt run` failed at 15:55:02.178708 after 0.03 seconds
[0m15:55:02.178853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cc7820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ce9fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d4d8c0>]}
[0m15:55:02.178965 [debug] [MainThread]: Flushing usage events
[0m15:55:03.222676 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:55:36.308501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051e8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067af750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067af9d0>]}


============================== 15:55:36.310341 | 3cbdc9fe-69c4-4d31-8927-47ba9416ec83 ==============================
[0m15:55:36.310341 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:55:36.310595 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run', 'log_cache_events': 'False', 'printer_width': '80', 'introspect': 'True', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'write_json': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_format': 'default', 'static_parser': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'cache_selected_only': 'False', 'empty': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m15:55:36.312606 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_CLIENT_ID'
[0m15:55:36.313610 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.028894458, "process_in_blocks": "0", "process_kernel_time": 0.063018, "process_mem_max_rss": "114900992", "process_out_blocks": "0", "process_user_time": 0.430208}
[0m15:55:36.313793 [debug] [MainThread]: Command `dbt run` failed at 15:55:36.313756 after 0.03 seconds
[0m15:55:36.313906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067c7820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067e9fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10704d8c0>]}
[0m15:55:36.314017 [debug] [MainThread]: Flushing usage events
[0m15:55:37.330108 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:55:51.592592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aacad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100af750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100af9d0>]}


============================== 15:55:51.594445 | 3249f6ea-8fce-4059-b51f-601f4bbb2693 ==============================
[0m15:55:51.594445 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:55:51.594707 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error': 'None', 'quiet': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'invocation_command': 'dbt run', 'printer_width': '80', 'partial_parse': 'True', 'target_path': 'None', 'use_colors': 'True', 'log_format': 'default', 'empty': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'debug': 'False', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'write_json': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False'}
[0m15:55:51.870075 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:55:51.870349 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:55:51.870482 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:55:52.121368 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m15:55:52.999071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3249f6ea-8fce-4059-b51f-601f4bbb2693', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ecd350>]}
[0m15:55:53.019111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3249f6ea-8fce-4059-b51f-601f4bbb2693', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c808d0>]}
[0m15:55:53.019350 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m15:55:53.578507 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 3 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m15:55:53.580700 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.0117278, "process_in_blocks": "0", "process_kernel_time": 0.176751, "process_mem_max_rss": "241188864", "process_out_blocks": "0", "process_user_time": 1.013931}
[0m15:55:53.580942 [debug] [MainThread]: Command `dbt run` failed at 15:55:53.580895 after 2.01 seconds
[0m15:55:53.581204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122f09b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122f09c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123d54320>]}
[0m15:55:53.581361 [debug] [MainThread]: Flushing usage events
[0m15:55:54.833071 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:56:07.301338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105288ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110fa7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110fa79d0>]}


============================== 15:56:07.303161 | 51516b45-8174-4e27-8d82-09d0bd8d045d ==============================
[0m15:56:07.303161 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:56:07.303415 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'quiet': 'False', 'warn_error': 'None', 'empty': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'partial_parse': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'fail_fast': 'False', 'printer_width': '80', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'log_cache_events': 'False', 'invocation_command': 'dbt deps', 'cache_selected_only': 'False', 'use_colors': 'True'}
[0m15:56:07.341548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51516b45-8174-4e27-8d82-09d0bd8d045d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057d5350>]}
[0m15:56:07.346689 [debug] [MainThread]: Set downloads directory='/var/folders/6f/8qf3_spd7kg7jstndtz6t53m0000gn/T/dbt-downloads-2p88tma6'
[0m15:56:07.346907 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:56:07.411497 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:56:07.412256 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:56:07.460134 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:56:07.463469 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/dbt-labs/dbt-utils.git b03949bb53de3439f6a8057088ba4032"
[0m15:56:08.640919 [debug] [MainThread]: STDOUT: "b''"
[0m15:56:08.641339 [debug] [MainThread]: STDERR: "b"Cloning into 'b03949bb53de3439f6a8057088ba4032'...\n""
[0m15:56:08.641616 [debug] [MainThread]: Pulling new dependency b03949bb53de3439f6a8057088ba4032.
[0m15:56:08.641775 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:56:08.651457 [debug] [MainThread]: STDOUT: "b'ef562bac8583a3dd041437d1a0ed926d356da4fd\n'"
[0m15:56:08.651748 [debug] [MainThread]: STDERR: "b''"
[0m15:56:08.651891 [debug] [MainThread]: Checking out revision 1.1.1.
[0m15:56:08.652032 [debug] [MainThread]: Executing "git remote set-branches origin 1.1.1"
[0m15:56:08.660561 [debug] [MainThread]: STDOUT: "b''"
[0m15:56:08.660807 [debug] [MainThread]: STDERR: "b''"
[0m15:56:08.660915 [debug] [MainThread]: Executing "git fetch origin --depth 1 --tags 1.1.1"
[0m15:56:09.767104 [debug] [MainThread]: STDOUT: "b''"
[0m15:56:09.767595 [debug] [MainThread]: STDERR: "b'From https://github.com/dbt-labs/dbt-utils\n * tag               1.1.1      -> FETCH_HEAD\n * [new tag]         0.0.1      -> 0.0.1\n * [new tag]         0.1.0      -> 0.1.0\n * [new tag]         0.1.1      -> 0.1.1\n * [new tag]         0.1.10     -> 0.1.10\n * [new tag]         0.1.11     -> 0.1.11\n * [new tag]         0.1.12     -> 0.1.12\n * [new tag]         0.1.13     -> 0.1.13\n * [new tag]         0.1.14     -> 0.1.14\n * [new tag]         0.1.15     -> 0.1.15\n * [new tag]         0.1.16     -> 0.1.16\n * [new tag]         0.1.17     -> 0.1.17\n * [new tag]         0.1.18     -> 0.1.18\n * [new tag]         0.1.19     -> 0.1.19\n * [new tag]         0.1.2      -> 0.1.2\n * [new tag]         0.1.20     -> 0.1.20\n * [new tag]         0.1.21     -> 0.1.21\n * [new tag]         0.1.22     -> 0.1.22\n * [new tag]         0.1.23     -> 0.1.23\n * [new tag]         0.1.24     -> 0.1.24\n * [new tag]         0.1.25     -> 0.1.25\n * [new tag]         0.1.3      -> 0.1.3\n * [new tag]         0.1.4      -> 0.1.4\n * [new tag]         0.1.5      -> 0.1.5\n * [new tag]         0.1.6      -> 0.1.6\n * [new tag]         0.1.7      -> 0.1.7\n * [new tag]         0.1.8      -> 0.1.8\n * [new tag]         0.1.9      -> 0.1.9\n * [new tag]         0.2.0      -> 0.2.0\n * [new tag]         0.2.1      -> 0.2.1\n * [new tag]         0.2.2      -> 0.2.2\n * [new tag]         0.2.3      -> 0.2.3\n * [new tag]         0.2.4      -> 0.2.4\n * [new tag]         0.2.5      -> 0.2.5\n * [new tag]         0.3.0      -> 0.3.0\n * [new tag]         0.4.0      -> 0.4.0\n * [new tag]         0.4.1      -> 0.4.1\n * [new tag]         0.5.0      -> 0.5.0\n * [new tag]         0.5.1      -> 0.5.1\n * [new tag]         0.6.0      -> 0.6.0\n * [new tag]         0.6.1      -> 0.6.1\n * [new tag]         0.6.2      -> 0.6.2\n * [new tag]         0.6.3      -> 0.6.3\n * [new tag]         0.6.4      -> 0.6.4\n * [new tag]         0.6.5      -> 0.6.5\n * [new tag]         0.6.6      -> 0.6.6\n * [new tag]         0.7.0      -> 0.7.0\n * [new tag]         0.7.1      -> 0.7.1\n * [new tag]         0.7.2      -> 0.7.2\n * [new tag]         0.7.3      -> 0.7.3\n * [new tag]         0.7.4      -> 0.7.4\n * [new tag]         0.7.4-b1   -> 0.7.4-b1\n * [new tag]         0.7.4b1    -> 0.7.4b1\n * [new tag]         0.7.5      -> 0.7.5\n * [new tag]         0.7.6      -> 0.7.6\n * [new tag]         0.8.0      -> 0.8.0\n * [new tag]         0.8.1      -> 0.8.1\n * [new tag]         0.8.2      -> 0.8.2\n * [new tag]         0.8.3      -> 0.8.3\n * [new tag]         0.8.4      -> 0.8.4\n * [new tag]         0.8.5      -> 0.8.5\n * [new tag]         0.8.6      -> 0.8.6\n * [new tag]         0.9.0      -> 0.9.0\n * [new tag]         0.9.1      -> 0.9.1\n * [new tag]         0.9.2      -> 0.9.2\n * [new tag]         0.9.5      -> 0.9.5\n * [new tag]         0.9.6      -> 0.9.6\n * [new tag]         1.0.0      -> 1.0.0\n * [new tag]         1.0.0-b1   -> 1.0.0-b1\n * [new tag]         1.0.0-b2   -> 1.0.0-b2\n * [new tag]         1.0.0-rc1  -> 1.0.0-rc1\n * [new tag]         1.1.0      -> 1.1.0\n * [new tag]         1.1.1      -> 1.1.1\n * [new tag]         1.2.0      -> 1.2.0\n * [new tag]         1.2.0-rc1  -> 1.2.0-rc1\n * [new tag]         1.3.0      -> 1.3.0\n * [new tag]         1.3.1      -> 1.3.1\n * [new tag]         1.3.2      -> 1.3.2\n'"
[0m15:56:09.767823 [debug] [MainThread]: Executing "git tag --list"
[0m15:56:09.778326 [debug] [MainThread]: STDOUT: "b'0.0.1\n0.1.0\n0.1.1\n0.1.10\n0.1.11\n0.1.12\n0.1.13\n0.1.14\n0.1.15\n0.1.16\n0.1.17\n0.1.18\n0.1.19\n0.1.2\n0.1.20\n0.1.21\n0.1.22\n0.1.23\n0.1.24\n0.1.25\n0.1.3\n0.1.4\n0.1.5\n0.1.6\n0.1.7\n0.1.8\n0.1.9\n0.2.0\n0.2.1\n0.2.2\n0.2.3\n0.2.4\n0.2.5\n0.3.0\n0.4.0\n0.4.1\n0.5.0\n0.5.1\n0.6.0\n0.6.1\n0.6.2\n0.6.3\n0.6.4\n0.6.5\n0.6.6\n0.7.0\n0.7.1\n0.7.2\n0.7.3\n0.7.4\n0.7.4-b1\n0.7.4b1\n0.7.5\n0.7.6\n0.8.0\n0.8.1\n0.8.2\n0.8.3\n0.8.4\n0.8.5\n0.8.6\n0.9.0\n0.9.1\n0.9.2\n0.9.5\n0.9.6\n1.0.0\n1.0.0-b1\n1.0.0-b2\n1.0.0-rc1\n1.1.0\n1.1.1\n1.2.0\n1.2.0-rc1\n1.3.0\n1.3.1\n1.3.2\n1.3.3\n'"
[0m15:56:09.778646 [debug] [MainThread]: STDERR: "b''"
[0m15:56:09.778801 [debug] [MainThread]: Executing "git reset --hard tags/1.1.1"
[0m15:56:09.800086 [debug] [MainThread]: STDOUT: "b'HEAD is now at 74a661c Update CHANGELOG for v1.1.1 (#803)\n'"
[0m15:56:09.800377 [debug] [MainThread]: STDERR: "b''"
[0m15:56:09.800508 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:56:09.807698 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:56:09.807912 [debug] [MainThread]: STDERR: "b''"
[0m15:56:09.808020 [debug] [MainThread]: Checked out at 74a661c.
[0m15:56:09.808128 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:56:09.814485 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:56:09.814694 [debug] [MainThread]: STDERR: "b''"
[0m15:56:09.815228 [error] [MainThread]: Encountered an error:
Runtime Error
  No dbt_project.yml found at expected path /opt/dbt/bigquery/dbt_project.yml
  Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml
  
[0m15:56:09.816322 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 2.5388265, "process_in_blocks": "0", "process_kernel_time": 0.093588, "process_mem_max_rss": "126271488", "process_out_blocks": "0", "process_user_time": 0.519692}
[0m15:56:09.816518 [debug] [MainThread]: Command `dbt deps` failed at 15:56:09.816479 after 2.54 seconds
[0m15:56:09.816679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110abac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110a9480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111117450>]}
[0m15:56:09.816820 [debug] [MainThread]: Flushing usage events
[0m15:56:11.217266 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:25.648277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108484ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2a7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2a79d0>]}


============================== 15:57:25.650116 | fc925391-abd5-4f64-a436-89f02c53b809 ==============================
[0m15:57:25.650116 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:57:25.650367 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'warn_error': 'None', 'introspect': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'indirect_selection': 'eager', 'empty': 'None', 'version_check': 'True', 'use_colors': 'True', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt deps', 'no_print': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'debug': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'printer_width': '80', 'static_parser': 'True', 'quiet': 'False', 'log_cache_events': 'False'}
[0m15:57:25.687954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc925391-abd5-4f64-a436-89f02c53b809', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091d1350>]}
[0m15:57:25.693159 [debug] [MainThread]: Set downloads directory='/var/folders/6f/8qf3_spd7kg7jstndtz6t53m0000gn/T/dbt-downloads-u6woglkq'
[0m15:57:25.693369 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:57:25.756681 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:57:25.757442 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:57:25.808141 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:57:25.811516 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/dbt-labs/dbt-utils.git b03949bb53de3439f6a8057088ba4032"
[0m15:57:26.879406 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:26.879863 [debug] [MainThread]: STDERR: "b"Cloning into 'b03949bb53de3439f6a8057088ba4032'...\n""
[0m15:57:26.880171 [debug] [MainThread]: Pulling new dependency b03949bb53de3439f6a8057088ba4032.
[0m15:57:26.880339 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:26.890479 [debug] [MainThread]: STDOUT: "b'ef562bac8583a3dd041437d1a0ed926d356da4fd\n'"
[0m15:57:26.890753 [debug] [MainThread]: STDERR: "b''"
[0m15:57:26.890895 [debug] [MainThread]: Checking out revision 1.1.1.
[0m15:57:26.891023 [debug] [MainThread]: Executing "git remote set-branches origin 1.1.1"
[0m15:57:26.899643 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:26.899901 [debug] [MainThread]: STDERR: "b''"
[0m15:57:26.900015 [debug] [MainThread]: Executing "git fetch origin --depth 1 --tags 1.1.1"
[0m15:57:28.132163 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:28.132706 [debug] [MainThread]: STDERR: "b'From https://github.com/dbt-labs/dbt-utils\n * tag               1.1.1      -> FETCH_HEAD\n * [new tag]         0.0.1      -> 0.0.1\n * [new tag]         0.1.0      -> 0.1.0\n * [new tag]         0.1.1      -> 0.1.1\n * [new tag]         0.1.10     -> 0.1.10\n * [new tag]         0.1.11     -> 0.1.11\n * [new tag]         0.1.12     -> 0.1.12\n * [new tag]         0.1.13     -> 0.1.13\n * [new tag]         0.1.14     -> 0.1.14\n * [new tag]         0.1.15     -> 0.1.15\n * [new tag]         0.1.16     -> 0.1.16\n * [new tag]         0.1.17     -> 0.1.17\n * [new tag]         0.1.18     -> 0.1.18\n * [new tag]         0.1.19     -> 0.1.19\n * [new tag]         0.1.2      -> 0.1.2\n * [new tag]         0.1.20     -> 0.1.20\n * [new tag]         0.1.21     -> 0.1.21\n * [new tag]         0.1.22     -> 0.1.22\n * [new tag]         0.1.23     -> 0.1.23\n * [new tag]         0.1.24     -> 0.1.24\n * [new tag]         0.1.25     -> 0.1.25\n * [new tag]         0.1.3      -> 0.1.3\n * [new tag]         0.1.4      -> 0.1.4\n * [new tag]         0.1.5      -> 0.1.5\n * [new tag]         0.1.6      -> 0.1.6\n * [new tag]         0.1.7      -> 0.1.7\n * [new tag]         0.1.8      -> 0.1.8\n * [new tag]         0.1.9      -> 0.1.9\n * [new tag]         0.2.0      -> 0.2.0\n * [new tag]         0.2.1      -> 0.2.1\n * [new tag]         0.2.2      -> 0.2.2\n * [new tag]         0.2.3      -> 0.2.3\n * [new tag]         0.2.4      -> 0.2.4\n * [new tag]         0.2.5      -> 0.2.5\n * [new tag]         0.3.0      -> 0.3.0\n * [new tag]         0.4.0      -> 0.4.0\n * [new tag]         0.4.1      -> 0.4.1\n * [new tag]         0.5.0      -> 0.5.0\n * [new tag]         0.5.1      -> 0.5.1\n * [new tag]         0.6.0      -> 0.6.0\n * [new tag]         0.6.1      -> 0.6.1\n * [new tag]         0.6.2      -> 0.6.2\n * [new tag]         0.6.3      -> 0.6.3\n * [new tag]         0.6.4      -> 0.6.4\n * [new tag]         0.6.5      -> 0.6.5\n * [new tag]         0.6.6      -> 0.6.6\n * [new tag]         0.7.0      -> 0.7.0\n * [new tag]         0.7.1      -> 0.7.1\n * [new tag]         0.7.2      -> 0.7.2\n * [new tag]         0.7.3      -> 0.7.3\n * [new tag]         0.7.4      -> 0.7.4\n * [new tag]         0.7.4-b1   -> 0.7.4-b1\n * [new tag]         0.7.4b1    -> 0.7.4b1\n * [new tag]         0.7.5      -> 0.7.5\n * [new tag]         0.7.6      -> 0.7.6\n * [new tag]         0.8.0      -> 0.8.0\n * [new tag]         0.8.1      -> 0.8.1\n * [new tag]         0.8.2      -> 0.8.2\n * [new tag]         0.8.3      -> 0.8.3\n * [new tag]         0.8.4      -> 0.8.4\n * [new tag]         0.8.5      -> 0.8.5\n * [new tag]         0.8.6      -> 0.8.6\n * [new tag]         0.9.0      -> 0.9.0\n * [new tag]         0.9.1      -> 0.9.1\n * [new tag]         0.9.2      -> 0.9.2\n * [new tag]         0.9.5      -> 0.9.5\n * [new tag]         0.9.6      -> 0.9.6\n * [new tag]         1.0.0      -> 1.0.0\n * [new tag]         1.0.0-b1   -> 1.0.0-b1\n * [new tag]         1.0.0-b2   -> 1.0.0-b2\n * [new tag]         1.0.0-rc1  -> 1.0.0-rc1\n * [new tag]         1.1.0      -> 1.1.0\n * [new tag]         1.1.1      -> 1.1.1\n * [new tag]         1.2.0      -> 1.2.0\n * [new tag]         1.2.0-rc1  -> 1.2.0-rc1\n * [new tag]         1.3.0      -> 1.3.0\n * [new tag]         1.3.1      -> 1.3.1\n * [new tag]         1.3.2      -> 1.3.2\n'"
[0m15:57:28.132949 [debug] [MainThread]: Executing "git tag --list"
[0m15:57:28.143627 [debug] [MainThread]: STDOUT: "b'0.0.1\n0.1.0\n0.1.1\n0.1.10\n0.1.11\n0.1.12\n0.1.13\n0.1.14\n0.1.15\n0.1.16\n0.1.17\n0.1.18\n0.1.19\n0.1.2\n0.1.20\n0.1.21\n0.1.22\n0.1.23\n0.1.24\n0.1.25\n0.1.3\n0.1.4\n0.1.5\n0.1.6\n0.1.7\n0.1.8\n0.1.9\n0.2.0\n0.2.1\n0.2.2\n0.2.3\n0.2.4\n0.2.5\n0.3.0\n0.4.0\n0.4.1\n0.5.0\n0.5.1\n0.6.0\n0.6.1\n0.6.2\n0.6.3\n0.6.4\n0.6.5\n0.6.6\n0.7.0\n0.7.1\n0.7.2\n0.7.3\n0.7.4\n0.7.4-b1\n0.7.4b1\n0.7.5\n0.7.6\n0.8.0\n0.8.1\n0.8.2\n0.8.3\n0.8.4\n0.8.5\n0.8.6\n0.9.0\n0.9.1\n0.9.2\n0.9.5\n0.9.6\n1.0.0\n1.0.0-b1\n1.0.0-b2\n1.0.0-rc1\n1.1.0\n1.1.1\n1.2.0\n1.2.0-rc1\n1.3.0\n1.3.1\n1.3.2\n1.3.3\n'"
[0m15:57:28.143942 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.144097 [debug] [MainThread]: Executing "git reset --hard tags/1.1.1"
[0m15:57:28.165675 [debug] [MainThread]: STDOUT: "b'HEAD is now at 74a661c Update CHANGELOG for v1.1.1 (#803)\n'"
[0m15:57:28.165985 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.166127 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:28.173369 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:57:28.173601 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.173720 [debug] [MainThread]: Checked out at 74a661c.
[0m15:57:28.173828 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:28.180307 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:57:28.180518 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.181912 [debug] [MainThread]: Executing "git clone --depth 1 https://github.com/dbt-labs/dbt-utils.git b03949bb53de3439f6a8057088ba4032"
[0m15:57:28.188572 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:28.188787 [debug] [MainThread]: STDERR: "b"fatal: destination path 'b03949bb53de3439f6a8057088ba4032' already exists and is not an empty directory.\n""
[0m15:57:28.188874 [debug] [MainThread]: command return code=128
[0m15:57:28.189198 [debug] [MainThread]: Updating existing dependency b03949bb53de3439f6a8057088ba4032.
[0m15:57:28.189296 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:28.195769 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:57:28.195974 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.196069 [debug] [MainThread]: Checking out revision 1.1.1.
[0m15:57:28.196164 [debug] [MainThread]: Executing "git remote set-branches origin 1.1.1"
[0m15:57:28.203485 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:28.203776 [debug] [MainThread]: STDERR: "b''"
[0m15:57:28.203891 [debug] [MainThread]: Executing "git fetch origin --depth 1 --tags 1.1.1"
[0m15:57:29.070752 [debug] [MainThread]: STDOUT: "b''"
[0m15:57:29.071392 [debug] [MainThread]: STDERR: "b'From https://github.com/dbt-labs/dbt-utils\n * tag               1.1.1      -> FETCH_HEAD\n'"
[0m15:57:29.071604 [debug] [MainThread]: Executing "git tag --list"
[0m15:57:29.085283 [debug] [MainThread]: STDOUT: "b'0.0.1\n0.1.0\n0.1.1\n0.1.10\n0.1.11\n0.1.12\n0.1.13\n0.1.14\n0.1.15\n0.1.16\n0.1.17\n0.1.18\n0.1.19\n0.1.2\n0.1.20\n0.1.21\n0.1.22\n0.1.23\n0.1.24\n0.1.25\n0.1.3\n0.1.4\n0.1.5\n0.1.6\n0.1.7\n0.1.8\n0.1.9\n0.2.0\n0.2.1\n0.2.2\n0.2.3\n0.2.4\n0.2.5\n0.3.0\n0.4.0\n0.4.1\n0.5.0\n0.5.1\n0.6.0\n0.6.1\n0.6.2\n0.6.3\n0.6.4\n0.6.5\n0.6.6\n0.7.0\n0.7.1\n0.7.2\n0.7.3\n0.7.4\n0.7.4-b1\n0.7.4b1\n0.7.5\n0.7.6\n0.8.0\n0.8.1\n0.8.2\n0.8.3\n0.8.4\n0.8.5\n0.8.6\n0.9.0\n0.9.1\n0.9.2\n0.9.5\n0.9.6\n1.0.0\n1.0.0-b1\n1.0.0-b2\n1.0.0-rc1\n1.1.0\n1.1.1\n1.2.0\n1.2.0-rc1\n1.3.0\n1.3.1\n1.3.2\n1.3.3\n'"
[0m15:57:29.085666 [debug] [MainThread]: STDERR: "b''"
[0m15:57:29.085851 [debug] [MainThread]: Executing "git reset --hard tags/1.1.1"
[0m15:57:29.100404 [debug] [MainThread]: STDOUT: "b'HEAD is now at 74a661c Update CHANGELOG for v1.1.1 (#803)\n'"
[0m15:57:29.100740 [debug] [MainThread]: STDERR: "b''"
[0m15:57:29.100906 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:29.108959 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:57:29.109230 [debug] [MainThread]: STDERR: "b''"
[0m15:57:29.109368 [debug] [MainThread]: Already at 74a661c, nothing to do.
[0m15:57:29.109493 [debug] [MainThread]: Executing "git rev-parse HEAD"
[0m15:57:29.117007 [debug] [MainThread]: STDOUT: "b'74a661ca00452da7d2b2811ff1e5a37f152c3ee2\n'"
[0m15:57:29.117256 [debug] [MainThread]: STDERR: "b''"
[0m15:57:29.117825 [error] [MainThread]: Encountered an error:
Runtime Error
  Found duplicate project "dbt_utils". This occurs when a dependency has the same project name as some other dependency.
[0m15:57:29.119060 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 3.4947329, "process_in_blocks": "0", "process_kernel_time": 0.1123, "process_mem_max_rss": "123420672", "process_out_blocks": "0", "process_user_time": 0.526575}
[0m15:57:29.119300 [debug] [MainThread]: Command `dbt deps` failed at 15:57:29.119250 after 3.50 seconds
[0m15:57:29.119476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3abac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3a9480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a48b450>]}
[0m15:57:29.119644 [debug] [MainThread]: Flushing usage events
[0m15:57:30.502310 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:50.404960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109588ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba79d0>]}


============================== 15:57:50.406802 | e928ae44-77eb-484a-91b3-99f2d18fb324 ==============================
[0m15:57:50.406802 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:57:50.407043 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'warn_error': 'None', 'empty': 'None', 'fail_fast': 'False', 'introspect': 'True', 'printer_width': '80', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'cache_selected_only': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt deps', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_format': 'default', 'version_check': 'True', 'log_cache_events': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'quiet': 'False', 'no_print': 'None', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs'}
[0m15:57:50.448871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e928ae44-77eb-484a-91b3-99f2d18fb324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109abd350>]}
[0m15:57:50.454462 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m15:57:50.454963 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m15:57:50.456070 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.074921, "process_in_blocks": "0", "process_kernel_time": 0.072809, "process_mem_max_rss": "117030912", "process_out_blocks": "0", "process_user_time": 0.475628}
[0m15:57:50.456382 [debug] [MainThread]: Command `dbt deps` succeeded at 15:57:50.456304 after 0.08 seconds
[0m15:57:50.456545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aca8e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aca9480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c178150>]}
[0m15:57:50.456677 [debug] [MainThread]: Flushing usage events
[0m15:57:51.699156 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:58:00.631601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103facad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105eaf750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105eaf9d0>]}


============================== 15:58:00.633395 | 0ba4bc79-82e9-4596-8aed-76931d0650e9 ==============================
[0m15:58:00.633395 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:58:00.633631 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run', 'empty': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'quiet': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'log_format': 'default', 'version_check': 'True', 'fail_fast': 'False', 'debug': 'False', 'target_path': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m15:58:00.898036 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:58:00.898259 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:58:00.898373 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:58:01.127830 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m15:58:02.019134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045d1350>]}
[0m15:58:02.040255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b808d0>]}
[0m15:58:02.040501 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m15:58:02.089600 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m15:58:02.089922 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:58:02.090072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e705e50>]}
[0m15:58:02.579026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1038bb6b0>]}
[0m15:58:02.598354 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m15:58:02.599068 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m15:58:02.602000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f306cf0>]}
[0m15:58:02.602153 [info ] [MainThread]: Found 1 model, 709 macros
[0m15:58:02.602278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ba4bc79-82e9-4596-8aed-76931d0650e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b7efc70>]}
[0m15:58:02.602891 [info ] [MainThread]: 
[0m15:58:02.603020 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:58:02.603118 [info ] [MainThread]: 
[0m15:58:02.603308 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:58:02.603410 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:58:02.603729 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m15:58:02.603880 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m15:58:02.607032 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m15:58:02.607165 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m15:58:02.607265 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:58:03.449766 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m15:58:03.476614 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.8210768699645996/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m15:58:03.477080 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m15:58:03.477376 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m15:58:03.477660 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m15:58:03.478104 [info ] [MainThread]: 
[0m15:58:03.478380 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.87 seconds (0.87s).
[0m15:58:03.478671 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m15:58:03.481541 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.8737576, "process_in_blocks": "0", "process_kernel_time": 0.178, "process_mem_max_rss": "251822080", "process_out_blocks": "0", "process_user_time": 1.578455}
[0m15:58:03.481794 [debug] [MainThread]: Command `dbt run` failed at 15:58:03.481742 after 2.87 seconds
[0m15:58:03.482002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105da0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5526d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b552780>]}
[0m15:58:03.482194 [debug] [MainThread]: Flushing usage events
[0m15:58:04.677179 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:59:41.379067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a84ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3af750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3af9d0>]}


============================== 15:59:41.380966 | a93c91fc-3d31-4f47-8ab8-7ac53350e7b2 ==============================
[0m15:59:41.380966 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:59:41.381224 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'version_check': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'introspect': 'True', 'no_print': 'None', 'printer_width': '80', 'cache_selected_only': 'False', 'quiet': 'False', 'write_json': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'log_format': 'default', 'use_colors': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'warn_error': 'None', 'log_cache_events': 'False'}
[0m15:59:41.646492 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:59:41.646713 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:59:41.646826 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:59:41.866140 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m15:59:42.737008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a93c91fc-3d31-4f47-8ab8-7ac53350e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fd5350>]}
[0m15:59:42.758704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a93c91fc-3d31-4f47-8ab8-7ac53350e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9808d0>]}
[0m15:59:42.758978 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m15:59:42.809075 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m15:59:42.839960 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:59:42.840123 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:59:42.840222 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:59:42.848624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a93c91fc-3d31-4f47-8ab8-7ac53350e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da06750>]}
[0m15:59:42.867866 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m15:59:42.868639 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m15:59:42.871666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a93c91fc-3d31-4f47-8ab8-7ac53350e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d99b890>]}
[0m15:59:42.871827 [info ] [MainThread]: Found 1 model, 709 macros
[0m15:59:42.871952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a93c91fc-3d31-4f47-8ab8-7ac53350e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da5bcb0>]}
[0m15:59:42.872562 [info ] [MainThread]: 
[0m15:59:42.872683 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:59:42.872779 [info ] [MainThread]: 
[0m15:59:42.872991 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:59:42.873098 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:59:42.873454 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m15:59:42.873623 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m15:59:42.877565 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m15:59:42.877719 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m15:59:42.877832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:43.694230 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m15:59:43.720609 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.7948670387268066/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m15:59:43.721029 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m15:59:43.721298 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m15:59:43.721542 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m15:59:43.721967 [info ] [MainThread]: 
[0m15:59:43.722170 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.85 seconds (0.85s).
[0m15:59:43.722443 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m15:59:43.725203 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.3701336, "process_in_blocks": "0", "process_kernel_time": 0.175332, "process_mem_max_rss": "252641280", "process_out_blocks": "0", "process_user_time": 1.138971}
[0m15:59:43.725459 [debug] [MainThread]: Command `dbt run` failed at 15:59:43.725404 after 2.37 seconds
[0m15:59:43.725667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fb48bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fafb350>]}
[0m15:59:43.725849 [debug] [MainThread]: Flushing usage events
[0m15:59:44.927477 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:59:46.882993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118decad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aeb7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aeb79d0>]}


============================== 15:59:46.885039 | 1dede027-f9a5-48bc-b5d4-373e9e4c7370 ==============================
[0m15:59:46.885039 [info ] [MainThread]: Running with dbt=1.11.2
[0m15:59:46.885294 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'fail_fast': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'target_path': 'None', 'log_format': 'default', 'introspect': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'partial_parse': 'True', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'use_colors': 'True', 'indirect_selection': 'eager', 'printer_width': '80'}
[0m15:59:47.156386 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:59:47.158920 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:59:47.159077 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:59:47.379219 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m15:59:48.234840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1dede027-f9a5-48bc-b5d4-373e9e4c7370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1193d1350>]}
[0m15:59:48.254653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1dede027-f9a5-48bc-b5d4-373e9e4c7370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb808d0>]}
[0m15:59:48.254890 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m15:59:48.302839 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m15:59:48.333923 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:59:48.334088 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:59:48.334190 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:59:48.342558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1dede027-f9a5-48bc-b5d4-373e9e4c7370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d406750>]}
[0m15:59:48.361943 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m15:59:48.362678 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m15:59:48.365466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1dede027-f9a5-48bc-b5d4-373e9e4c7370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d39b890>]}
[0m15:59:48.365631 [info ] [MainThread]: Found 1 model, 709 macros
[0m15:59:48.365757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1dede027-f9a5-48bc-b5d4-373e9e4c7370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d45bcb0>]}
[0m15:59:48.366372 [info ] [MainThread]: 
[0m15:59:48.366496 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:59:48.366593 [info ] [MainThread]: 
[0m15:59:48.366792 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:59:48.366899 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:59:48.367246 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m15:59:48.367410 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m15:59:48.371201 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m15:59:48.371320 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m15:59:48.371412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:49.191120 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m15:59:49.216071 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.7978620529174805/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m15:59:49.216466 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m15:59:49.216739 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m15:59:49.216988 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m15:59:49.217416 [info ] [MainThread]: 
[0m15:59:49.217619 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.85 seconds (0.85s).
[0m15:59:49.217883 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m15:59:49.220587 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.3634565, "process_in_blocks": "0", "process_kernel_time": 0.179264, "process_mem_max_rss": "247185408", "process_out_blocks": "0", "process_user_time": 1.133463}
[0m15:59:49.220850 [debug] [MainThread]: Command `dbt run` failed at 15:59:49.220798 after 2.36 seconds
[0m15:59:49.221059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee48bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ada0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edff350>]}
[0m15:59:49.221248 [debug] [MainThread]: Flushing usage events
[0m15:59:50.354630 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:02:30.052827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a60ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104997750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049979d0>]}


============================== 16:02:30.054799 | 8a409f2c-e395-4bc2-a51b-d7d6f715ca97 ==============================
[0m16:02:30.054799 [info ] [MainThread]: Running with dbt=1.11.2
[0m16:02:30.055055 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'warn_error': 'None', 'printer_width': '80', 'cache_selected_only': 'False', 'no_print': 'None', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'log_format': 'default', 'introspect': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'write_json': 'True', 'quiet': 'False', 'target_path': 'None'}
[0m16:02:30.320297 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:02:30.320525 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:02:30.320637 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:02:30.543275 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:02:31.427079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a409f2c-e395-4bc2-a51b-d7d6f715ca97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103cbd350>]}
[0m16:02:31.448236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a409f2c-e395-4bc2-a51b-d7d6f715ca97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f688d0>]}
[0m16:02:31.448483 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m16:02:31.497133 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m16:02:31.528049 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:02:31.528212 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:02:31.528309 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:02:31.536678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a409f2c-e395-4bc2-a51b-d7d6f715ca97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115406750>]}
[0m16:02:31.555696 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:02:31.556426 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:02:31.559219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a409f2c-e395-4bc2-a51b-d7d6f715ca97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11529b890>]}
[0m16:02:31.559367 [info ] [MainThread]: Found 1 model, 709 macros
[0m16:02:31.559493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a409f2c-e395-4bc2-a51b-d7d6f715ca97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11545bcb0>]}
[0m16:02:31.560056 [info ] [MainThread]: 
[0m16:02:31.560188 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:02:31.560285 [info ] [MainThread]: 
[0m16:02:31.560491 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:02:31.560600 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:02:31.560940 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:02:31.561109 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:02:31.564911 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:02:31.565039 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m16:02:31.565139 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:32.390480 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m16:02:32.417499 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.8036589622497559/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m16:02:32.417970 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m16:02:32.418262 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m16:02:32.418528 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m16:02:32.419008 [info ] [MainThread]: 
[0m16:02:32.419208 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.86 seconds (0.86s).
[0m16:02:32.419480 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m16:02:32.422330 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.3931227, "process_in_blocks": "0", "process_kernel_time": 0.18017, "process_mem_max_rss": "247185408", "process_out_blocks": "0", "process_user_time": 1.144978}
[0m16:02:32.422605 [debug] [MainThread]: Command `dbt run` failed at 16:02:32.422553 after 2.39 seconds
[0m16:02:32.422807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b48bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104888950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117af7350>]}
[0m16:02:32.422996 [debug] [MainThread]: Flushing usage events
[0m16:02:33.586337 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:03:18.536327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105858ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10678f750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10678f9d0>]}


============================== 16:03:18.538194 | 26c542db-0595-40e6-99c2-fae2d703f231 ==============================
[0m16:03:18.538194 [info ] [MainThread]: Running with dbt=1.11.2
[0m16:03:18.538448 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'introspect': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'target_path': 'None', 'use_experimental_parser': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'warn_error': 'None', 'write_json': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'empty': 'False', 'no_print': 'None'}
[0m16:03:18.803180 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:03:18.803392 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:03:18.803506 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:03:19.023598 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:03:19.886246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '26c542db-0595-40e6-99c2-fae2d703f231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ab1350>]}
[0m16:03:19.906246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '26c542db-0595-40e6-99c2-fae2d703f231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080808d0>]}
[0m16:03:19.906485 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m16:03:19.953227 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m16:03:19.984243 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:03:19.984399 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:03:19.984496 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:03:19.992891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26c542db-0595-40e6-99c2-fae2d703f231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff06750>]}
[0m16:03:20.012014 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:03:20.012749 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:03:20.015550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26c542db-0595-40e6-99c2-fae2d703f231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe9b890>]}
[0m16:03:20.015700 [info ] [MainThread]: Found 1 model, 709 macros
[0m16:03:20.015825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26c542db-0595-40e6-99c2-fae2d703f231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff5bcb0>]}
[0m16:03:20.016438 [info ] [MainThread]: 
[0m16:03:20.016559 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:03:20.016658 [info ] [MainThread]: 
[0m16:03:20.016861 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:03:20.016970 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:03:20.017316 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:03:20.017472 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:03:20.021265 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:03:20.021395 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m16:03:20.021495 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:03:20.839974 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m16:03:20.866861 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.797766923904419/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m16:03:20.867269 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m16:03:20.867532 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m16:03:20.867774 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m16:03:20.868199 [info ] [MainThread]: 
[0m16:03:20.868402 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.85 seconds (0.85s).
[0m16:03:20.868673 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m16:03:20.871457 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.359126, "process_in_blocks": "0", "process_kernel_time": 0.173667, "process_mem_max_rss": "246628352", "process_out_blocks": "0", "process_user_time": 1.132757}
[0m16:03:20.871720 [debug] [MainThread]: Command `dbt run` failed at 16:03:20.871670 after 2.36 seconds
[0m16:03:20.871920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a248bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106680950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1f7350>]}
[0m16:03:20.872119 [debug] [MainThread]: Flushing usage events
[0m16:03:22.046533 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:07:32.504512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106344ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100af750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100af9d0>]}


============================== 16:07:32.506387 | 23bd0c9b-ef42-4de7-b2ca-2abd0437203c ==============================
[0m16:07:32.506387 [info ] [MainThread]: Running with dbt=1.11.2
[0m16:07:32.506639 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'debug': 'False', 'partial_parse': 'True', 'target_path': 'None', 'printer_width': '80', 'use_experimental_parser': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'warn_error': 'None', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'indirect_selection': 'eager', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'log_cache_events': 'False', 'version_check': 'True', 'log_format': 'default'}
[0m16:07:32.782169 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:07:32.782411 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:07:32.782544 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:07:33.004459 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:07:33.990753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '23bd0c9b-ef42-4de7-b2ca-2abd0437203c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067d1350>]}
[0m16:07:34.011984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '23bd0c9b-ef42-4de7-b2ca-2abd0437203c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149808d0>]}
[0m16:07:34.012284 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m16:07:34.061196 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m16:07:34.092318 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:07:34.092490 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:07:34.092587 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:07:34.100995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '23bd0c9b-ef42-4de7-b2ca-2abd0437203c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124406750>]}
[0m16:07:34.120196 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:07:34.120916 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:07:34.123709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '23bd0c9b-ef42-4de7-b2ca-2abd0437203c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12429b890>]}
[0m16:07:34.123858 [info ] [MainThread]: Found 1 model, 709 macros
[0m16:07:34.123980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23bd0c9b-ef42-4de7-b2ca-2abd0437203c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12445bcb0>]}
[0m16:07:34.124585 [info ] [MainThread]: 
[0m16:07:34.124706 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:07:34.124803 [info ] [MainThread]: 
[0m16:07:34.125007 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:07:34.125112 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:07:34.125473 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:07:34.125637 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:07:34.129651 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:07:34.129810 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m16:07:34.129928 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:07:34.130075 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:07:34.130227 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:07:34.130511 [info ] [MainThread]: 
[0m16:07:34.130633 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:07:34.130799 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:07:34.132650 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.6524124, "process_in_blocks": "0", "process_kernel_time": 0.174542, "process_mem_max_rss": "246448128", "process_out_blocks": "0", "process_user_time": 1.071795}
[0m16:07:34.132820 [debug] [MainThread]: Command `dbt run` failed at 16:07:34.132787 after 1.65 seconds
[0m16:07:34.133035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125348bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fa0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1252fb110>]}
[0m16:07:34.133396 [debug] [MainThread]: Flushing usage events
[0m16:07:35.358560 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:09:27.345968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b088ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c6b7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c6b79d0>]}


============================== 16:09:27.347832 | f5a3fd4e-7a2c-4594-b577-ab2af22cb15e ==============================
[0m16:09:27.347832 [info ] [MainThread]: Running with dbt=1.11.2
[0m16:09:27.348087 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'target_path': 'None', 'log_format': 'default', 'no_print': 'None', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'empty': 'False', 'quiet': 'False', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False'}
[0m16:09:27.609547 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:09:27.609767 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:09:27.609879 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:09:27.826683 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:09:28.704265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5a3fd4e-7a2c-4594-b577-ab2af22cb15e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5bd350>]}
[0m16:09:28.725331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5a3fd4e-7a2c-4594-b577-ab2af22cb15e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb808d0>]}
[0m16:09:28.725578 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m16:09:28.775434 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m16:09:28.805295 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:09:28.805459 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:09:28.805557 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:09:28.814005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5a3fd4e-7a2c-4594-b577-ab2af22cb15e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f406750>]}
[0m16:09:28.833176 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:09:28.833880 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:09:28.836656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5a3fd4e-7a2c-4594-b577-ab2af22cb15e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f39f890>]}
[0m16:09:28.836816 [info ] [MainThread]: Found 1 model, 709 macros
[0m16:09:28.836944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5a3fd4e-7a2c-4594-b577-ab2af22cb15e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f45bcb0>]}
[0m16:09:28.837560 [info ] [MainThread]: 
[0m16:09:28.837694 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:09:28.837794 [info ] [MainThread]: 
[0m16:09:28.838010 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:09:28.838123 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:09:28.838489 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:09:28.838659 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:09:28.842509 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:09:28.842638 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
[0m16:09:28.842736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:09:28.842860 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.0", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    show databases
  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:09:28.843020 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:09:28.843340 [info ] [MainThread]: 
[0m16:09:28.843523 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:09:28.843714 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:09:28.845535 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5235504, "process_in_blocks": "0", "process_kernel_time": 0.155551, "process_mem_max_rss": "245252096", "process_out_blocks": "0", "process_user_time": 1.073276}
[0m16:09:28.845714 [debug] [MainThread]: Command `dbt run` failed at 16:09:28.845679 after 1.52 seconds
[0m16:09:28.845857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd4cbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c5a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fcfb110>]}
[0m16:09:28.845991 [debug] [MainThread]: Flushing usage events
[0m16:09:30.047491 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:10:20.548194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10967cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaa7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaa79d0>]}


============================== 16:10:20.550182 | 1575c414-a2c3-4a70-bc0c-b67b71e6fbfa ==============================
[0m16:10:20.550182 [info ] [MainThread]: Running with dbt=1.11.2
[0m16:10:20.550427 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False', 'static_parser': 'True', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'fail_fast': 'False', 'target_path': 'None', 'introspect': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'invocation_command': 'dbt debug', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'use_colors': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'log_format': 'default', 'empty': 'None'}
[0m16:10:20.553928 [info ] [MainThread]: dbt version: 1.11.2
[0m16:10:20.554070 [info ] [MainThread]: python version: 3.13.11
[0m16:10:20.554181 [info ] [MainThread]: python path: /Users/fahadm/anaconda3/envs/iot_simulator_datalake/bin/python3.13
[0m16:10:20.554289 [info ] [MainThread]: os info: macOS-26.2-arm64-arm-64bit-Mach-O
[0m16:10:20.818410 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:10:20.818619 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:10:20.818729 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:10:21.039821 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:10:21.850789 [info ] [MainThread]: Using profiles dir at /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake
[0m16:10:21.851406 [info ] [MainThread]: Using profiles.yml file at /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/profiles.yml
[0m16:10:21.851718 [info ] [MainThread]: Using dbt_project.yml file at /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/dbt_project.yml
[0m16:10:21.851981 [info ] [MainThread]: adapter type: databricks
[0m16:10:21.852224 [info ] [MainThread]: adapter version: 1.11.0
[0m16:10:21.902316 [info ] [MainThread]: Configuration:
[0m16:10:21.902591 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m16:10:21.902738 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:10:21.902857 [info ] [MainThread]: Required dependencies:
[0m16:10:21.903054 [debug] [MainThread]: Executing "git --help"
[0m16:10:21.917125 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:10:21.917666 [debug] [MainThread]: STDERR: "b''"
[0m16:10:21.917833 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:10:21.917985 [info ] [MainThread]: Connection:
[0m16:10:21.918147 [info ] [MainThread]:   host: dbc-6ee8b0d3-4446.cloud.databricks.com
[0m16:10:21.918277 [info ] [MainThread]:   http_path: sql/protocolv1/o/7474658012323014/1226-041156-cefvjcb6
[0m16:10:21.918389 [info ] [MainThread]:   catalog: dev_catalog
[0m16:10:21.918494 [info ] [MainThread]:   schema: bronze
[0m16:10:21.918831 [info ] [MainThread]: Registered adapter: databricks=1.11.0
[0m16:10:21.965197 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m16:10:21.965457 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m16:10:21.965576 [debug] [MainThread]: Using databricks connection "debug"
[0m16:10:21.965676 [debug] [MainThread]: On debug: select 1 as id
[0m16:10:21.965769 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:10:21.965914 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
select 1 as id
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:10:21.966098 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m16:10:21.966225 [info ] [MainThread]: [31m1 check failed:[0m
[0m16:10:21.966348 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m16:10:21.968066 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.4447794, "process_in_blocks": "0", "process_kernel_time": 0.14995, "process_mem_max_rss": "238387200", "process_out_blocks": "0", "process_user_time": 0.986174}
[0m16:10:21.968249 [debug] [MainThread]: Command `dbt debug` failed at 16:10:21.968210 after 1.45 seconds
[0m16:10:21.968427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b3822c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b3f8ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2206b0>]}
[0m16:10:21.968580 [debug] [MainThread]: Flushing usage events
[0m16:10:23.193568 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:45.327839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fd0ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f0b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f0b890>]}


============================== 16:14:45.329504 | 285d106a-c32c-4b46-9dea-9dc262763cf6 ==============================
[0m16:14:45.329504 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:14:45.329746 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'introspect': 'True', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'empty': 'False', 'log_format': 'default', 'no_print': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'target_path': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'fail_fast': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'debug': 'False'}
[0m16:14:45.604615 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:14:45.604837 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:14:45.604962 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:14:45.823982 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:14:46.728472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108221220>]}
[0m16:14:46.749386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c67c380>]}
[0m16:14:46.749655 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:14:46.796199 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:14:46.796589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114701550>]}
[0m16:14:46.800859 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:14:46.825653 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m16:14:46.825893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145a3020>]}
[0m16:14:47.310188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d41c50>]}
[0m16:14:47.329742 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:14:47.330435 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:14:47.333209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c1e680>]}
[0m16:14:47.333360 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:14:47.333489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '285d106a-c32c-4b46-9dea-9dc262763cf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150fa990>]}
[0m16:14:47.334101 [info ] [MainThread]: 
[0m16:14:47.334229 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:14:47.334332 [info ] [MainThread]: 
[0m16:14:47.334533 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:14:47.334641 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:14:47.334975 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:14:47.335158 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:14:47.339275 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:14:47.339429 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:14:47.339550 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:47.339699 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:14:47.339852 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:14:47.340139 [info ] [MainThread]: 
[0m16:14:47.340265 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:14:47.340437 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:14:47.342387 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.0386436, "process_in_blocks": "0", "process_kernel_time": 0.174351, "process_mem_max_rss": "249856000", "process_out_blocks": "0", "process_user_time": 1.516339}
[0m16:14:47.342574 [debug] [MainThread]: Command `dbt run` failed at 16:14:47.342540 after 2.04 seconds
[0m16:14:47.342716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114fc8310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bfadf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bfbcf0>]}
[0m16:14:47.342847 [debug] [MainThread]: Flushing usage events
[0m16:14:48.618508 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:20:52.031723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111584ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aaf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aaf890>]}


============================== 16:20:52.033521 | 400fe6de-2658-49b6-b14d-1164f60d0d1c ==============================
[0m16:20:52.033521 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:20:52.033778 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'use_colors': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'use_experimental_parser': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'empty': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'debug': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'write_json': 'True', 'quiet': 'False', 'target_path': 'None', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'partial_parse': 'True', 'printer_width': '80'}
[0m16:20:52.308480 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:20:52.308724 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:20:52.308833 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:20:52.544617 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:20:53.544241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119d5220>]}
[0m16:20:53.562328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a78380>]}
[0m16:20:53.562556 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:20:53.606218 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:20:53.606590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133f05550>]}
[0m16:20:53.610812 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:20:53.646977 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:20:53.647176 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:20:53.647274 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:20:53.656057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135abaa80>]}
[0m16:20:53.675927 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:20:53.676690 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:20:53.679633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133f57bd0>]}
[0m16:20:53.679797 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:20:53.679926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '400fe6de-2658-49b6-b14d-1164f60d0d1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135cf7c70>]}
[0m16:20:53.680541 [info ] [MainThread]: 
[0m16:20:53.680666 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:20:53.680765 [info ] [MainThread]: 
[0m16:20:53.680968 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:20:53.681084 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:20:53.681429 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:20:53.681582 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:20:53.686057 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:20:53.686188 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:20:53.686294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:20:53.686421 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:20:53.686552 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:20:53.686843 [info ] [MainThread]: 
[0m16:20:53.687028 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:20:53.687219 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:20:53.689264 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.6823174, "process_in_blocks": "0", "process_kernel_time": 0.177345, "process_mem_max_rss": "247218176", "process_out_blocks": "0", "process_user_time": 1.054583}
[0m16:20:53.689467 [debug] [MainThread]: Command `dbt run` failed at 16:20:53.689431 after 1.68 seconds
[0m16:20:53.689617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135e34520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135e345d0>]}
[0m16:20:53.689733 [debug] [MainThread]: Flushing usage events
[0m16:20:54.907966 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:21:06.617146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110278ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149af9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149af890>]}


============================== 16:21:06.618792 | fec3404d-1c27-4516-921a-b41bea27b0e4 ==============================
[0m16:21:06.618792 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:21:06.619032 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'log_format': 'default', 'write_json': 'True', 'static_parser': 'True', 'introspect': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'cache_selected_only': 'False', 'version_check': 'True', 'fail_fast': 'False', 'printer_width': '80', 'invocation_command': 'dbt run', 'target_path': 'None', 'empty': 'False', 'no_print': 'None', 'use_colors': 'True', 'debug': 'False', 'partial_parse': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager'}
[0m16:21:06.881503 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:21:06.881730 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:21:06.881843 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:21:07.102139 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:21:07.953796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ad1220>]}
[0m16:21:07.971638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d7c380>]}
[0m16:21:07.971860 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:21:08.017054 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:21:08.017459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126701550>]}
[0m16:21:08.021844 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:21:08.054125 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:21:08.054363 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:21:08.054458 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:21:08.062953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126fbaa80>]}
[0m16:21:08.083324 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:21:08.084102 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:21:08.086950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126757bd0>]}
[0m16:21:08.087107 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:21:08.087234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fec3404d-1c27-4516-921a-b41bea27b0e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1301f3c70>]}
[0m16:21:08.087857 [info ] [MainThread]: 
[0m16:21:08.087985 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:21:08.088084 [info ] [MainThread]: 
[0m16:21:08.088306 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:21:08.088416 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:21:08.088775 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:21:08.088936 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:21:08.093842 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:21:08.094007 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:21:08.094114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:08.938480 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m16:21:08.965903 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.822878360748291/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m16:21:08.966343 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m16:21:08.966629 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m16:21:08.966894 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m16:21:08.967362 [info ] [MainThread]: 
[0m16:21:08.967643 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.88 seconds (0.88s).
[0m16:21:08.967945 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m16:21:08.970745 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.3778377, "process_in_blocks": "0", "process_kernel_time": 0.167111, "process_mem_max_rss": "248446976", "process_out_blocks": "0", "process_user_time": 1.135044}
[0m16:21:08.971019 [debug] [MainThread]: Command `dbt run` failed at 16:21:08.970966 after 2.38 seconds
[0m16:21:08.971223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130228680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130228730>]}
[0m16:21:08.971422 [debug] [MainThread]: Flushing usage events
[0m16:21:09.961407 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:22:27.354763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a78ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112eaf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112eaf890>]}


============================== 16:22:27.356402 | ff17bb55-8c0b-4930-bbb3-37e4fd27e24a ==============================
[0m16:22:27.356402 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:22:27.356656 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'empty': 'False', 'profiles_dir': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'static_parser': 'True', 'introspect': 'True', 'warn_error': 'None', 'target_path': 'None', 'debug': 'False', 'use_colors': 'True', 'printer_width': '80', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'version_check': 'True', 'write_json': 'True', 'log_format': 'default', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m16:22:27.623036 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:22:27.623278 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:22:27.623394 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:22:27.849300 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:22:28.756014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ebd220>]}
[0m16:22:28.777145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11537c380>]}
[0m16:22:28.777391 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:22:28.822485 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:22:28.822840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124c05550>]}
[0m16:22:28.827072 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:22:28.859792 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:22:28.859993 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:22:28.860087 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:22:28.868645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264baa80>]}
[0m16:22:28.888171 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:22:28.888903 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:22:28.891737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124c57bd0>]}
[0m16:22:28.891896 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:22:28.892020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff17bb55-8c0b-4930-bbb3-37e4fd27e24a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1278f3c70>]}
[0m16:22:28.892620 [info ] [MainThread]: 
[0m16:22:28.892743 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:22:28.892839 [info ] [MainThread]: 
[0m16:22:28.893048 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:22:28.893154 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:22:28.893502 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:22:28.893684 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:22:28.898896 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:22:28.899099 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:22:28.899228 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:22:28.899381 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:22:28.899513 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:22:28.899790 [info ] [MainThread]: 
[0m16:22:28.899915 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:22:28.900077 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:22:28.902054 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5716162, "process_in_blocks": "0", "process_kernel_time": 0.175545, "process_mem_max_rss": "244531200", "process_out_blocks": "0", "process_user_time": 1.076193}
[0m16:22:28.902296 [debug] [MainThread]: Command `dbt run` failed at 16:22:28.902251 after 1.57 seconds
[0m16:22:28.902456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112da0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127a34520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127a345d0>]}
[0m16:22:28.902600 [debug] [MainThread]: Flushing usage events
[0m16:22:30.039283 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:23:50.150933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059bcad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106eaf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106eaf890>]}


============================== 16:23:50.152606 | fe1c1351-3ea0-4292-b8d1-b28758bedee5 ==============================
[0m16:23:50.152606 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:23:50.152863 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'log_cache_events': 'False', 'printer_width': '80', 'debug': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'introspect': 'True', 'version_check': 'True', 'empty': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/fahadm/.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'partial_parse': 'True', 'log_format': 'default', 'warn_error': 'None', 'target_path': 'None'}
[0m16:23:50.421314 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:23:50.421547 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:23:50.421661 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:23:50.662252 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:23:51.545588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dbd220>]}
[0m16:23:51.565729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d78380>]}
[0m16:23:51.565971 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:23:51.610370 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:23:51.610735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba05550>]}
[0m16:23:51.614946 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:23:51.647306 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:23:51.647531 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:23:51.647621 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:23:51.655997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfbaa80>]}
[0m16:23:51.675359 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:23:51.676109 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:23:51.678896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba57bd0>]}
[0m16:23:51.679046 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:23:51.679168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe1c1351-3ea0-4292-b8d1-b28758bedee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9f7c70>]}
[0m16:23:51.679787 [info ] [MainThread]: 
[0m16:23:51.679915 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:23:51.680013 [info ] [MainThread]: 
[0m16:23:51.680217 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:23:51.680326 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:23:51.680685 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:23:51.680852 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:23:51.685507 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:23:51.685647 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:23:51.685755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:23:51.685892 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Runtime Error
  The config `auth_type: oauth` is required when not using access token
[0m16:23:51.686044 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Runtime Error
    The config `auth_type: oauth` is required when not using access token
[0m16:23:51.686376 [info ] [MainThread]: 
[0m16:23:51.686585 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m16:23:51.686780 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Runtime Error
      The config `auth_type: oauth` is required when not using access token
[0m16:23:51.688790 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5620632, "process_in_blocks": "0", "process_kernel_time": 0.180684, "process_mem_max_rss": "245874688", "process_out_blocks": "0", "process_user_time": 1.075051}
[0m16:23:51.689075 [debug] [MainThread]: Command `dbt run` failed at 16:23:51.688978 after 1.56 seconds
[0m16:23:51.689382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106da0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca34520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca345d0>]}
[0m16:23:51.689628 [debug] [MainThread]: Flushing usage events
[0m16:23:52.880717 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:23:59.301779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108074ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096af9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096af890>]}


============================== 16:23:59.303549 | 36b5d871-6d21-4c55-9304-c548cda382dd ==============================
[0m16:23:59.303549 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:23:59.303813 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'partial_parse': 'True', 'profiles_dir': '/Users/fahadm/.dbt', 'fail_fast': 'False', 'empty': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'static_parser': 'True', 'write_json': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'printer_width': '80'}
[0m16:23:59.565874 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:23:59.566111 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:23:59.566225 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:23:59.783942 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET
[0m16:24:00.638874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085bd220>]}
[0m16:24:00.659792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc78380>]}
[0m16:24:00.660037 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:24:00.704488 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:24:00.704869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b205550>]}
[0m16:24:00.709061 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:24:00.740559 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:24:00.740759 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:24:00.740852 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:24:00.749502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c0baa80>]}
[0m16:24:00.769388 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:24:00.770157 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:24:00.773013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b257bd0>]}
[0m16:24:00.773168 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:24:00.773286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36b5d871-6d21-4c55-9304-c548cda382dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c2f7c70>]}
[0m16:24:00.773862 [info ] [MainThread]: 
[0m16:24:00.773985 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:24:00.774083 [info ] [MainThread]: 
[0m16:24:00.774294 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:24:00.774400 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:24:00.774725 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:24:00.774869 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:24:00.779470 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:24:00.779611 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:24:00.779716 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:24:01.690237 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m16:24:01.717337 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.8893837928771973/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m16:24:01.717767 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m16:24:01.718040 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m16:24:01.718295 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m16:24:01.718802 [info ] [MainThread]: 
[0m16:24:01.719095 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.94 seconds (0.94s).
[0m16:24:01.719397 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m16:24:01.722280 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.4448903, "process_in_blocks": "0", "process_kernel_time": 0.162757, "process_mem_max_rss": "247808000", "process_out_blocks": "0", "process_user_time": 1.138942}
[0m16:24:01.722553 [debug] [MainThread]: Command `dbt run` failed at 16:24:01.722501 after 2.45 seconds
[0m16:24:01.722762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c32c680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c32c730>]}
[0m16:24:01.722964 [debug] [MainThread]: Flushing usage events
[0m16:24:02.916847 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:25:55.263405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10537cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aaf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aaf890>]}


============================== 16:25:55.265142 | d6a00e10-00ba-4512-b652-ddbe709fa1f0 ==============================
[0m16:25:55.265142 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:25:55.265383 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'invocation_command': 'dbt run', 'introspect': 'True', 'no_print': 'None', 'profiles_dir': '/Users/fahadm/.dbt', 'empty': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'printer_width': '80', 'use_colors': 'True', 'log_format': 'default', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs'}
[0m16:25:55.549378 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:25:55.549615 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:25:55.549731 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:25:55.770882 [warn ] [MainThread]: Databricks adapter: Failed to authenticate with legacy-azure-client-secret, trying oauth-m2m next. Error: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://dbc-6ee8b0d3-4446.cloud.databricks.com, client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, client_secret=***, profile=IOT_SIMULATOR_DATALAKE_DEV, azure_client_secret=***, azure_client_id=a1126402-7cc5-4067-99be-feb57b1d2b7c, auth_type=azure-client-secret. Env: DATABRICKS_HOST, DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET, DATABRICKS_CONFIG_PROFILE
[0m16:25:56.674089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d1220>]}
[0m16:25:56.695270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113278380>]}
[0m16:25:56.695523 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:25:56.740740 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:25:56.741114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133105550>]}
[0m16:25:56.745327 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:25:56.777680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:25:56.777876 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:25:56.777974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:25:56.786610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1337baa80>]}
[0m16:25:56.806065 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:25:56.806808 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:25:56.809722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133157bd0>]}
[0m16:25:56.809881 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:25:56.810004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6a00e10-00ba-4512-b652-ddbe709fa1f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1349f3c70>]}
[0m16:25:56.810617 [info ] [MainThread]: 
[0m16:25:56.810744 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:25:56.810842 [info ] [MainThread]: 
[0m16:25:56.811057 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:25:56.811169 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:25:56.811540 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:25:56.811699 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:25:56.816487 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:25:56.816652 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:25:56.816756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:25:57.710569 [error] [ThreadPool]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m16:25:57.736986 [error] [ThreadPool]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server. invalid_client: Client authentication failed
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=0.8729221820831299/900.0, error-message=, http-code=None, method=OpenSession, no-retry-reason=non-retryable error, original-exception=invalid_client: Client authentication failed, query-id=None, session-id=None
[0m16:25:57.737446 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
: Database Error
  Error during request to server. invalid_client: Client authentication failed
[0m16:25:57.737732 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  Database Error
    Error during request to server. invalid_client: Client authentication failed
[0m16:25:57.737996 [debug] [ThreadPool]: On list_dev_catalog: No close available on handle
[0m16:25:57.738475 [info ] [MainThread]: 
[0m16:25:57.738710 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.93 seconds (0.93s).
[0m16:25:57.738983 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    Database Error
      Error during request to server. invalid_client: Client authentication failed
[0m16:25:57.741764 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.5023706, "process_in_blocks": "0", "process_kernel_time": 0.182105, "process_mem_max_rss": "253329408", "process_out_blocks": "0", "process_user_time": 1.155235}
[0m16:25:57.742032 [debug] [MainThread]: Command `dbt run` failed at 16:25:57.741982 after 2.50 seconds
[0m16:25:57.742239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079a0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x134a34680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x134a34730>]}
[0m16:25:57.742427 [debug] [MainThread]: Flushing usage events
[0m16:25:58.994472 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:35:33.292403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104814ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105daf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105daf890>]}


============================== 16:35:33.294023 | ce8e94ab-68ca-4e20-8120-cde52ad796cb ==============================
[0m16:35:33.294023 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:35:33.294270 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'profiles_dir': '/Users/fahadm/.dbt', 'use_experimental_parser': 'False', 'no_print': 'None', 'invocation_command': 'dbt debug', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'empty': 'None', 'printer_width': '80', 'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'use_colors': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'static_parser': 'True'}
[0m16:35:33.298678 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m16:35:33.298844 [info ] [MainThread]: python version: 3.13.11
[0m16:35:33.298960 [info ] [MainThread]: python path: /Users/fahadm/anaconda3/envs/iot_simulator_datalake/bin/python3.13
[0m16:35:33.299061 [info ] [MainThread]: os info: macOS-26.2-arm64-arm-64bit-Mach-O
[0m16:35:33.300618 [info ] [MainThread]: target not specified in profile 'iot_simulator_datalake', using 'default'
[0m16:35:33.300840 [info ] [MainThread]: Using profiles dir at /Users/fahadm/.dbt
[0m16:35:33.300953 [info ] [MainThread]: Using profiles.yml file at /Users/fahadm/.dbt/profiles.yml
[0m16:35:33.301051 [info ] [MainThread]: Using dbt_project.yml file at /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/dbt_project.yml
[0m16:35:33.331591 [info ] [MainThread]: Configuration:
[0m16:35:33.331817 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m16:35:33.331917 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:35:33.332019 [info ] [MainThread]: Required dependencies:
[0m16:35:33.332180 [debug] [MainThread]: Executing "git --help"
[0m16:35:33.342877 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:35:33.343397 [debug] [MainThread]: STDERR: "b''"
[0m16:35:33.343531 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:35:33.343647 [info ] [MainThread]: Connection test skipped since no profile was found
[0m16:35:33.343748 [info ] [MainThread]: [31m1 check failed:[0m
[0m16:35:33.343833 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  The profile 'iot_simulator_datalake' does not have a target named 'default'. The valid target names for this profile are:
   - aws-oauth-u2m


[0m16:35:33.345061 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.07782875, "process_in_blocks": "0", "process_kernel_time": 0.076245, "process_mem_max_rss": "117407744", "process_out_blocks": "0", "process_user_time": 0.475585}
[0m16:35:33.345275 [debug] [MainThread]: Command `dbt debug` failed at 16:35:33.345237 after 0.08 seconds
[0m16:35:33.345451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b468b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f2e690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e44d10>]}
[0m16:35:33.345610 [debug] [MainThread]: Flushing usage events
[0m16:35:34.651770 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:37:05.555862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040b8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053af9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053af890>]}


============================== 16:37:05.557579 | aefaa7cd-6ee4-4763-bfc2-44715dbb34c4 ==============================
[0m16:37:05.557579 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:37:05.557832 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'quiet': 'False', 'empty': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'static_parser': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'profiles_dir': '/Users/fahadm/.dbt', 'cache_selected_only': 'False', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'no_print': 'None'}
[0m16:37:05.559139 [info ] [MainThread]: target not specified in profile 'iot_simulator_datalake', using 'default'
[0m16:37:05.559324 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'iot_simulator_datalake' does not have a target named 'default'. The valid target names for this profile are:
   - aws-oauth-u2m
[0m16:37:05.560337 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.028153708, "process_in_blocks": "0", "process_kernel_time": 0.07472, "process_mem_max_rss": "113999872", "process_out_blocks": "0", "process_user_time": 0.437}
[0m16:37:05.560519 [debug] [MainThread]: Command `dbt run` failed at 16:37:05.560484 after 0.03 seconds
[0m16:37:05.560635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053c63f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10550d010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10554c490>]}
[0m16:37:05.560768 [debug] [MainThread]: Flushing usage events
[0m16:37:06.725761 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:37:56.198275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111178ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112db79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112db7890>]}


============================== 16:37:56.199917 | 8761609b-9137-435a-8600-799285411faa ==============================
[0m16:37:56.199917 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:37:56.200169 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'target_path': 'None', 'use_colors': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'log_format': 'default', 'profiles_dir': '/Users/fahadm/.dbt', 'warn_error': 'None', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'fail_fast': 'False', 'invocation_command': 'dbt run -t aws-oauth-u2m', 'cache_selected_only': 'False', 'static_parser': 'True', 'empty': 'False', 'debug': 'False', 'quiet': 'False', 'partial_parse': 'True', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True'}
[0m16:37:56.465105 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:37:56.465318 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:37:56.465429 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:37:59.488388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bd1220>]}
[0m16:37:59.508757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244fb680>]}
[0m16:37:59.509037 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:37:59.552335 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:37:59.552711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124601a50>]}
[0m16:37:59.556828 [debug] [MainThread]: checksum: e7ec2a42d90c5ec5874d02435520609204535ad0a1d06aa209b27dd8353e4300, vars: {}, profile: , target: aws-oauth-u2m, version: 1.11.0rc3
[0m16:37:59.579377 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m16:37:59.579552 [debug] [MainThread]: previous checksum: e7ec2a42d90c5ec5874d02435520609204535ad0a1d06aa209b27dd8353e4300, current checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0
[0m16:37:59.579670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244a2c60>]}
[0m16:38:00.066448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125341d30>]}
[0m16:38:00.085491 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:38:00.086182 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:38:00.088887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125952680>]}
[0m16:38:00.089043 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:38:00.089172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1256fe990>]}
[0m16:38:00.089784 [info ] [MainThread]: 
[0m16:38:00.089912 [info ] [MainThread]: Concurrency: 1 threads (target='aws-oauth-u2m')
[0m16:38:00.090012 [info ] [MainThread]: 
[0m16:38:00.090204 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:38:00.090311 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:38:00.090637 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:38:00.090791 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:38:00.094801 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:38:00.094930 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:38:00.095048 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:43:03.670303 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7ff1a53c-5e8a-48ff-a7a8-75c0838b89c0) - Created
[0m16:44:14.020580 [debug] [ThreadPool]: SQL status: OK in 373.930 seconds
[0m16:44:14.023959 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7ff1a53c-5e8a-48ff-a7a8-75c0838b89c0, command-id=e6e7967a-60c5-46c1-b48c-e8cad61cc3b3) - Closing
[0m16:44:14.024800 [debug] [ThreadPool]: On list_dev_catalog: Close
[0m16:44:14.025171 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7ff1a53c-5e8a-48ff-a7a8-75c0838b89c0) - Closing
[0m16:44:14.263771 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog_bronze) - Creating connection
[0m16:44:14.264710 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog_bronze'
[0m16:44:14.276463 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog_bronze"
[0m16:44:14.276786 [debug] [ThreadPool]: On list_dev_catalog_bronze: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "connection_name": "list_dev_catalog_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev_catalog' 
  AND table_schema = 'bronze'

  
[0m16:44:14.277017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:44:15.248338 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=1d21c1d6-e84f-4b1b-861d-3ecf4793497c) - Created
[0m16:44:22.286271 [debug] [ThreadPool]: SQL status: OK in 8.010 seconds
[0m16:44:22.290463 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1d21c1d6-e84f-4b1b-861d-3ecf4793497c, command-id=90b67516-3464-4adf-9926-5366ee317c02) - Closing
[0m16:44:22.516507 [debug] [ThreadPool]: On list_dev_catalog_bronze: Close
[0m16:44:22.517766 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=1d21c1d6-e84f-4b1b-861d-3ecf4793497c) - Closing
[0m16:44:22.752294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1255a5020>]}
[0m16:44:22.758058 [debug] [Thread-3 (]: Began running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:44:22.758783 [info ] [Thread-3 (]: 1 of 1 START sql view model bronze.stg_ingest_iot_events ....................... [RUN]
[0m16:44:22.759429 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.iot_simulator_datalake.stg_ingest_iot_events) - Creating connection
[0m16:44:22.759726 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.iot_simulator_datalake.stg_ingest_iot_events'
[0m16:44:22.759993 [debug] [Thread-3 (]: Began compiling node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:44:22.768033 [debug] [Thread-3 (]: Writing injected SQL for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:44:22.769186 [debug] [Thread-3 (]: Began executing node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:44:22.778326 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:44:22.782531 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:44:22.782833 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de9a90>]}
[0m16:44:22.789817 [debug] [Thread-3 (]: Creating view `dev_catalog`.`bronze`.`stg_ingest_iot_events`
[0m16:44:22.794581 [debug] [Thread-3 (]: Writing runtime sql for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:44:22.795189 [debug] [Thread-3 (]: Using databricks connection "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:44:22.795383 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "node_id": "model.iot_simulator_datalake.stg_ingest_iot_events"} */

  
  
  create or replace view `dev_catalog`.`bronze`.`stg_ingest_iot_events`
  
  as (
    select
    *
from read_files(
    '/Volumes/dev_catalog/landing/vol01/iot_events/iot_events/',
    FORMAT => JSON
)
LIMIT 2
  )

[0m16:44:22.795552 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:44:23.747123 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=c1b097c2-9eb0-4186-a6e7-a203d54f1608) - Created
[0m16:44:24.465869 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "node_id": "model.iot_simulator_datalake.stg_ingest_iot_events"} */

  
  
  create or replace view `dev_catalog`.`bronze`.`stg_ingest_iot_events`
  
  as (
    select
    *
from read_files(
    '/Volumes/dev_catalog/landing/vol01/iot_events/iot_events/',
    FORMAT => JSON
)
LIMIT 2
  )

: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=c72e2373-d5fb-4f6c-9424-7b52c5ec77db
[0m16:44:24.467341 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: Close
[0m16:44:24.467850 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=c1b097c2-9eb0-4186-a6e7-a203d54f1608) - Closing
[0m16:44:24.717864 [debug] [Thread-3 (]: Database Error in model stg_ingest_iot_events (models/staging/iot_events/stg_ingest_iot_events.sql)
  [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
  compiled code at target/run/iot_simulator_datalake/models/staging/iot_events/stg_ingest_iot_events.sql
[0m16:44:24.720086 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8761609b-9137-435a-8600-799285411faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b6cc50>]}
[0m16:44:24.720809 [error] [Thread-3 (]: 1 of 1 ERROR creating sql view model bronze.stg_ingest_iot_events .............. [[31mERROR[0m in 1.96s]
[0m16:44:24.721344 [debug] [Thread-3 (]: Finished running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:44:24.721821 [debug] [Thread-6 (]: Marking all children of 'model.iot_simulator_datalake.stg_ingest_iot_events' to be skipped because of status 'error'.  Reason: Database Error in model stg_ingest_iot_events (models/staging/iot_events/stg_ingest_iot_events.sql)
  [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
  compiled code at target/run/iot_simulator_datalake/models/staging/iot_events/stg_ingest_iot_events.sql.
[0m16:44:24.723409 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:44:24.723717 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:44:24.724039 [info ] [MainThread]: 
[0m16:44:24.724280 [info ] [MainThread]: Finished running 1 view model in 0 hours 6 minutes and 24.63 seconds (384.63s).
[0m16:44:24.724733 [debug] [MainThread]: Command end result
[0m16:44:24.740488 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:44:24.741471 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:44:24.744511 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/run_results.json
[0m16:44:24.744678 [info ] [MainThread]: 
[0m16:44:24.744856 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:44:24.745006 [info ] [MainThread]: 
[0m16:44:24.745184 [error] [MainThread]: [31mFailure in model stg_ingest_iot_events (models/staging/iot_events/stg_ingest_iot_events.sql)[0m
[0m16:44:24.745367 [error] [MainThread]:   Database Error in model stg_ingest_iot_events (models/staging/iot_events/stg_ingest_iot_events.sql)
  [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `JSON` cannot be resolved.  SQLSTATE: 42703; line 12 pos 14
  compiled code at target/run/iot_simulator_datalake/models/staging/iot_events/stg_ingest_iot_events.sql
[0m16:44:24.745508 [info ] [MainThread]: 
[0m16:44:24.745667 [info ] [MainThread]:   compiled code at target/compiled/iot_simulator_datalake/models/staging/iot_events/stg_ingest_iot_events.sql
[0m16:44:24.745805 [info ] [MainThread]: 
[0m16:44:24.745955 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m16:44:24.748443 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 388.57504, "process_in_blocks": "0", "process_kernel_time": 0.219229, "process_mem_max_rss": "263864320", "process_out_blocks": "0", "process_user_time": 1.989315}
[0m16:44:24.748665 [debug] [MainThread]: Command `dbt run` failed at 16:44:24.748624 after 388.58 seconds
[0m16:44:24.748843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de56a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1011de890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b860f0>]}
[0m16:44:24.749002 [debug] [MainThread]: Flushing usage events
[0m16:44:25.897195 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:44:53.543279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045ccad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055079d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105507890>]}


============================== 16:44:53.544925 | 73c1a61a-8194-4dcc-a3a6-75be8322f12a ==============================
[0m16:44:53.544925 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:44:53.545177 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'fail_fast': 'False', 'quiet': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run -t aws-oauth-u2m', 'empty': 'False', 'static_parser': 'True', 'no_print': 'None', 'partial_parse': 'True', 'profiles_dir': '/Users/fahadm/.dbt', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_format': 'default', 'printer_width': '80', 'introspect': 'True', 'debug': 'False', 'target_path': 'None', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:44:53.812733 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:44:53.812955 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:44:53.813072 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:44:54.934383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104829220>]}
[0m16:44:54.955020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10defb8a0>]}
[0m16:44:54.955270 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:44:55.000252 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:44:55.000639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e809750>]}
[0m16:44:55.005790 [debug] [MainThread]: checksum: e7ec2a42d90c5ec5874d02435520609204535ad0a1d06aa209b27dd8353e4300, vars: {}, profile: , target: aws-oauth-u2m, version: 1.11.0rc3
[0m16:44:55.038723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:44:55.038989 [debug] [MainThread]: Partial parsing: updated file: iot_simulator_datalake://models/staging/iot_events/stg_ingest_iot_events.sql
[0m16:44:55.108617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad7d040>]}
[0m16:44:55.128432 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:44:55.129231 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:44:55.132029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8b7850>]}
[0m16:44:55.132186 [info ] [MainThread]: Found 1 model, 715 macros
[0m16:44:55.132312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b85c120>]}
[0m16:44:55.133039 [info ] [MainThread]: 
[0m16:44:55.133207 [info ] [MainThread]: Concurrency: 1 threads (target='aws-oauth-u2m')
[0m16:44:55.133319 [info ] [MainThread]: 
[0m16:44:55.133533 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:44:55.133652 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:44:55.133988 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m16:44:55.134142 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m16:44:55.138119 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m16:44:55.138266 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m16:44:55.138385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:44:55.988055 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3306473e-3dcb-45df-ad33-abf969b305d9) - Created
[0m16:44:58.153626 [debug] [ThreadPool]: SQL status: OK in 3.020 seconds
[0m16:44:58.155975 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3306473e-3dcb-45df-ad33-abf969b305d9, command-id=59596bbe-d704-4882-9078-96885c16e7af) - Closing
[0m16:44:58.156608 [debug] [ThreadPool]: On list_dev_catalog: Close
[0m16:44:58.156920 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3306473e-3dcb-45df-ad33-abf969b305d9) - Closing
[0m16:44:58.388794 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog_bronze) - Creating connection
[0m16:44:58.389327 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog_bronze'
[0m16:44:58.398170 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog_bronze"
[0m16:44:58.398441 [debug] [ThreadPool]: On list_dev_catalog_bronze: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "connection_name": "list_dev_catalog_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev_catalog' 
  AND table_schema = 'bronze'

  
[0m16:44:58.398639 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:44:59.279342 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=cffb985d-fc10-496d-a346-77d2de738c3d) - Created
[0m16:45:00.205279 [debug] [ThreadPool]: SQL status: OK in 1.810 seconds
[0m16:45:00.209045 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=cffb985d-fc10-496d-a346-77d2de738c3d, command-id=28ff1716-ce40-467e-a59d-f0c8c5b8fd13) - Closing
[0m16:45:00.209832 [debug] [ThreadPool]: On list_dev_catalog_bronze: Close
[0m16:45:00.210039 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=cffb985d-fc10-496d-a346-77d2de738c3d) - Closing
[0m16:45:00.513655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a99cdd0>]}
[0m16:45:00.516153 [debug] [Thread-3 (]: Began running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:45:00.516664 [info ] [Thread-3 (]: 1 of 1 START sql view model bronze.stg_ingest_iot_events ....................... [RUN]
[0m16:45:00.517182 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.iot_simulator_datalake.stg_ingest_iot_events) - Creating connection
[0m16:45:00.517478 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.iot_simulator_datalake.stg_ingest_iot_events'
[0m16:45:00.517743 [debug] [Thread-3 (]: Began compiling node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:45:00.522572 [debug] [Thread-3 (]: Writing injected SQL for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:45:00.523234 [debug] [Thread-3 (]: Began executing node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:45:00.535183 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m16:45:00.539497 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:45:00.539795 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac52200>]}
[0m16:45:00.546545 [debug] [Thread-3 (]: Creating view `dev_catalog`.`bronze`.`stg_ingest_iot_events`
[0m16:45:00.551084 [debug] [Thread-3 (]: Writing runtime sql for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:45:00.551540 [debug] [Thread-3 (]: Using databricks connection "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m16:45:00.551868 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "aws-oauth-u2m", "node_id": "model.iot_simulator_datalake.stg_ingest_iot_events"} */

  
  
  create or replace view `dev_catalog`.`bronze`.`stg_ingest_iot_events`
  
  as (
    select
    *
from read_files(
    '/Volumes/dev_catalog/landing/vol01/iot_events/iot_events/',
    FORMAT => 'JSON'
)
LIMIT 2
  )

[0m16:45:00.552109 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:45:01.430805 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=48cf5ab9-8ad9-487b-88d3-b10fa5b869a2) - Created
[0m16:45:10.896265 [debug] [Thread-3 (]: SQL status: OK in 10.340 seconds
[0m16:45:10.898661 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=48cf5ab9-8ad9-487b-88d3-b10fa5b869a2, command-id=33e2c996-b7d6-405c-96a6-9c56539ac95c) - Closing
[0m16:45:11.137364 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:45:11.138789 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: Close
[0m16:45:11.138998 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=48cf5ab9-8ad9-487b-88d3-b10fa5b869a2) - Closing
[0m16:45:11.368280 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '73c1a61a-8194-4dcc-a3a6-75be8322f12a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047bb6b0>]}
[0m16:45:11.369165 [info ] [Thread-3 (]: 1 of 1 OK created sql view model bronze.stg_ingest_iot_events .................. [[32mOK[0m in 10.85s]
[0m16:45:11.369674 [debug] [Thread-3 (]: Finished running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m16:45:11.370932 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:45:11.371206 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:45:11.371547 [info ] [MainThread]: 
[0m16:45:11.371817 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 16.24 seconds (16.24s).
[0m16:45:11.372338 [debug] [MainThread]: Command end result
[0m16:45:11.389154 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m16:45:11.390431 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m16:45:11.393725 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/run_results.json
[0m16:45:11.393907 [info ] [MainThread]: 
[0m16:45:11.394112 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:45:11.394261 [info ] [MainThread]: 
[0m16:45:11.394425 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m16:45:11.396933 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 17.877443, "process_in_blocks": "0", "process_kernel_time": 0.19659, "process_mem_max_rss": "262979584", "process_out_blocks": "0", "process_user_time": 1.513013}
[0m16:45:11.397177 [debug] [MainThread]: Command `dbt run` succeeded at 16:45:11.397129 after 17.88 seconds
[0m16:45:11.397381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac6e350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11afbe1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055396a0>]}
[0m16:45:11.397560 [debug] [MainThread]: Flushing usage events
[0m16:45:12.413154 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:25:23.092157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123844ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124db79d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124db7890>]}


============================== 18:25:23.093692 | 0f12c2b8-9155-4929-8a0f-2a5b0d63d49a ==============================
[0m18:25:23.093692 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m18:25:23.093950 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'debug': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'profiles_dir': '/Users/fahadm/.dbt', 'use_colors': 'True', 'static_parser': 'True', 'introspect': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'no_print': 'None', 'warn_error': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'target_path': 'None', 'write_json': 'True', 'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs'}
[0m18:25:23.095290 [info ] [MainThread]: target not specified in profile 'iot_simulator_datalake', using 'default'
[0m18:25:23.095479 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'iot_simulator_datalake' does not have a target named 'default'. The valid target names for this profile are:
   - dev
[0m18:25:23.096473 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.028286375, "process_in_blocks": "0", "process_kernel_time": 0.072018, "process_mem_max_rss": "114737152", "process_out_blocks": "0", "process_user_time": 0.443042}
[0m18:25:23.096647 [debug] [MainThread]: Command `dbt run` failed at 18:25:23.096614 after 0.03 seconds
[0m18:25:23.096769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124dc23f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12590d010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12594c490>]}
[0m18:25:23.096882 [debug] [MainThread]: Flushing usage events
[0m18:25:24.259652 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:25:35.358722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ae4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061af9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061af890>]}


============================== 18:25:35.360321 | 4c004bb8-51c0-42be-8333-c1853fea03fd ==============================
[0m18:25:35.360321 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m18:25:35.360574 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/logs', 'empty': 'False', 'quiet': 'False', 'introspect': 'True', 'printer_width': '80', 'target_path': 'None', 'use_colors': 'True', 'profiles_dir': '/Users/fahadm/.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run -t dev', 'version_check': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_format': 'default', 'fail_fast': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False'}
[0m18:25:35.625113 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:25:35.625345 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:25:35.625463 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:25:37.670167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051c5220>]}
[0m18:25:37.691099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f2fb680>]}
[0m18:25:37.691357 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m18:25:37.734964 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:25:37.735358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f405750>]}
[0m18:25:37.739385 [debug] [MainThread]: checksum: 856869f1265e5b520acd7b85d1701e9b1ca7f597a9fd6cfc6c17cb756eb4d5ef, vars: {}, profile: , target: dev, version: 1.11.0rc3
[0m18:25:37.762935 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m18:25:37.763183 [debug] [MainThread]: previous checksum: 856869f1265e5b520acd7b85d1701e9b1ca7f597a9fd6cfc6c17cb756eb4d5ef, current checksum: e7ec2a42d90c5ec5874d02435520609204535ad0a1d06aa209b27dd8353e4300
[0m18:25:37.763294 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m18:25:37.763429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f2a2e40>]}
[0m18:25:38.244945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118b41c50>]}
[0m18:25:38.264211 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m18:25:38.265016 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m18:25:38.267805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a832680>]}
[0m18:25:38.267964 [info ] [MainThread]: Found 1 model, 715 macros
[0m18:25:38.268090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118efaa50>]}
[0m18:25:38.268686 [info ] [MainThread]: 
[0m18:25:38.268810 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:25:38.268910 [info ] [MainThread]: 
[0m18:25:38.269100 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:25:38.269218 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:25:38.269554 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog) - Creating connection
[0m18:25:38.269702 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog'
[0m18:25:38.273839 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog"
[0m18:25:38.273987 [debug] [ThreadPool]: On list_dev_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog"} */

    

  SHOW SCHEMAS IN `dev_catalog`


  
[0m18:25:38.274102 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:30:41.294603 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7a84bfa5-fe2b-42b5-bb6d-9057db913e1a) - Created
[0m18:31:51.038204 [debug] [ThreadPool]: SQL status: OK in 372.760 seconds
[0m18:31:51.041324 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7a84bfa5-fe2b-42b5-bb6d-9057db913e1a, command-id=7393228e-be1a-4eac-8dab-ca3b1982b14a) - Closing
[0m18:31:51.042155 [debug] [ThreadPool]: On list_dev_catalog: Close
[0m18:31:51.042458 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7a84bfa5-fe2b-42b5-bb6d-9057db913e1a) - Closing
[0m18:31:51.299719 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_catalog_bronze) - Creating connection
[0m18:31:51.300473 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_catalog_bronze'
[0m18:31:51.312700 [debug] [ThreadPool]: Using databricks connection "list_dev_catalog_bronze"
[0m18:31:51.313172 [debug] [ThreadPool]: On list_dev_catalog_bronze: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "connection_name": "list_dev_catalog_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev_catalog' 
  AND table_schema = 'bronze'

  
[0m18:31:51.313424 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:31:52.259383 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=4ce871a8-12fa-495c-9662-749bf48160f6) - Created
[0m18:31:57.063653 [debug] [ThreadPool]: SQL status: OK in 5.750 seconds
[0m18:31:57.066622 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ce871a8-12fa-495c-9662-749bf48160f6, command-id=175e02cf-1d4d-4f09-8d81-8e563023f89a) - Closing
[0m18:31:57.067508 [debug] [ThreadPool]: On list_dev_catalog_bronze: Close
[0m18:31:57.067762 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=4ce871a8-12fa-495c-9662-749bf48160f6) - Closing
[0m18:31:57.381620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f40f70>]}
[0m18:31:57.384862 [debug] [Thread-3 (]: Began running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m18:31:57.385733 [info ] [Thread-3 (]: 1 of 1 START sql table model bronze.stg_ingest_iot_events ...................... [RUN]
[0m18:31:57.386392 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.iot_simulator_datalake.stg_ingest_iot_events) - Creating connection
[0m18:31:57.386726 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.iot_simulator_datalake.stg_ingest_iot_events'
[0m18:31:57.387017 [debug] [Thread-3 (]: Began compiling node model.iot_simulator_datalake.stg_ingest_iot_events
[0m18:31:57.395786 [debug] [Thread-3 (]: Writing injected SQL for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m18:31:57.396537 [debug] [Thread-3 (]: Began executing node model.iot_simulator_datalake.stg_ingest_iot_events
[0m18:31:57.405336 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:31:57.408706 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:31:57.409092 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061e9a90>]}
[0m18:31:57.414886 [debug] [Thread-3 (]: Applying DROP to: `dev_catalog`.`bronze`.`stg_ingest_iot_events`
[0m18:31:57.417467 [debug] [Thread-3 (]: Using databricks connection "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m18:31:57.417710 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "node_id": "model.iot_simulator_datalake.stg_ingest_iot_events"} */
DROP VIEW IF EXISTS `dev_catalog`.`bronze`.`stg_ingest_iot_events`
[0m18:31:57.417896 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:58.299535 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=84c8cf27-923c-429f-8b59-ae6b6b81c6d5) - Created
[0m18:32:05.942007 [debug] [Thread-3 (]: SQL status: OK in 8.520 seconds
[0m18:32:05.943884 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=84c8cf27-923c-429f-8b59-ae6b6b81c6d5, command-id=e64900f1-9d43-40f1-a81a-3d0ce210282a) - Closing
[0m18:32:06.215146 [debug] [Thread-3 (]: Writing runtime sql for node "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m18:32:06.215749 [debug] [Thread-3 (]: Using databricks connection "model.iot_simulator_datalake.stg_ingest_iot_events"
[0m18:32:06.216004 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "iot_simulator_datalake", "target_name": "dev", "node_id": "model.iot_simulator_datalake.stg_ingest_iot_events"} */

  
    
        create or replace table `dev_catalog`.`bronze`.`stg_ingest_iot_events`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      select
    *
from read_files(
    '/Volumes/dev_catalog/landing/vol01/iot_events/iot_events/',
    FORMAT => 'JSON'
)
LIMIT 2
  
[0m18:32:19.809061 [debug] [Thread-3 (]: SQL status: OK in 13.590 seconds
[0m18:32:19.810221 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=84c8cf27-923c-429f-8b59-ae6b6b81c6d5, command-id=ab68dd1c-d89e-45ac-81ba-8ff694a28c58) - Closing
[0m18:32:20.048105 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:32:20.059961 [debug] [Thread-3 (]: On model.iot_simulator_datalake.stg_ingest_iot_events: Close
[0m18:32:20.060288 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=84c8cf27-923c-429f-8b59-ae6b6b81c6d5) - Closing
[0m18:32:20.292577 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4c004bb8-51c0-42be-8333-c1853fea03fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10516cc50>]}
[0m18:32:20.293953 [info ] [Thread-3 (]: 1 of 1 OK created sql table model bronze.stg_ingest_iot_events ................. [[32mOK[0m in 22.90s]
[0m18:32:20.294974 [debug] [Thread-3 (]: Finished running node model.iot_simulator_datalake.stg_ingest_iot_events
[0m18:32:20.296636 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:32:20.297089 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:32:20.297586 [info ] [MainThread]: 
[0m18:32:20.297948 [info ] [MainThread]: Finished running 1 table model in 0 hours 6 minutes and 42.03 seconds (402.03s).
[0m18:32:20.298583 [debug] [MainThread]: Command end result
[0m18:32:20.317082 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/manifest.json
[0m18:32:20.318182 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/semantic_manifest.json
[0m18:32:20.321400 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/fahadm/Documents/data/learning/spark/databricks/iot_simulator_datalake/target/run_results.json
[0m18:32:20.321591 [info ] [MainThread]: 
[0m18:32:20.321825 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:32:20.321985 [info ] [MainThread]: 
[0m18:32:20.322161 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m18:32:20.324820 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 404.99103, "process_in_blocks": "0", "process_kernel_time": 0.196995, "process_mem_max_rss": "260276224", "process_out_blocks": "0", "process_user_time": 2.051814}
[0m18:32:20.325054 [debug] [MainThread]: Command `dbt run` succeeded at 18:32:20.325008 after 404.99 seconds
[0m18:32:20.325245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061e56a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dda890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a65f10>]}
[0m18:32:20.325421 [debug] [MainThread]: Flushing usage events
[0m18:32:21.542714 [debug] [MainThread]: An error was encountered while trying to flush usage events
